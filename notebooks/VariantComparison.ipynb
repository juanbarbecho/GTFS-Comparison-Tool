{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8af01593-babf-48b2-8a12-9bd70282109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, shutil, zipfile, csv\n",
    "import requests\n",
    "import psycopg2\n",
    "import fiona\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString\n",
    "from collections import defaultdict\n",
    "from datetime import time\n",
    "\n",
    "from sqlalchemy import URL, create_engine, text\n",
    "from trino.auth import OAuth2Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07ae0f7e-08f2-4c1b-9ddc-d9eb8f896df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in parent directories to sys.path to get multiHook library\n",
    "current_dir = os.getcwd()\n",
    "for x in range(4):  # Look four levels up\n",
    "    parent_dir = os.path.dirname(current_dir)\n",
    "    if parent_dir not in sys.path:\n",
    "        sys.path.append(parent_dir)\n",
    "    current_dir = parent_dir\n",
    "\n",
    "import multihook.pycnxn.dbhook as dbhook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fb6ab1a-db80-479f-8738-b31bb32a9338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_bundle(\n",
    "    bundle,\n",
    "    import_trips=True,\n",
    "    import_shapes=True,\n",
    "    import_stops=True,\n",
    "    import_stoptimes=True,\n",
    "    import_calendar=True,\n",
    "    import_calendar_dates=True,\n",
    "    import_routes=True,\n",
    "    cache_bundle=False,\n",
    "):\n",
    "    \"\"\"Import GTFS data from the following tables in ADLS:\n",
    "    - core.dbo.fact_gtfs_trips\n",
    "    - core.dbo.fact_gtfs_shapes\n",
    "    - core.dbo.fact_gtfs_stops\n",
    "    - core.dbo.fact_gtfs_shape_reference\n",
    "    - core.dbo.fact_gtfs_stop_times\n",
    "\n",
    "    - ADD IN CALENDAR, CALENDAR DATES, AND ROUTES\n",
    "    \"\"\"\n",
    "\n",
    "    # GTFS bundle can take some time to import. If there is already a cached GTFS bundle pickle file in the repository, read in that data instead\n",
    "    picklefile = bundle + \".pickle\"\n",
    "    if os.path.isfile(picklefile) == True:\n",
    "        with open(picklefile, \"rb\") as handle:\n",
    "            gtfs_bundle = pickle.load(handle)\n",
    "            return gtfs_bundle\n",
    "\n",
    "    con = create_engine(\n",
    "        r\"trino://trino-route-trino.apps.mtasiprod.eastus.aroapp.io:443/mtadatalake\",\n",
    "        connect_args={\n",
    "            \"auth\": OAuth2Authentication(),\n",
    "            \"http_scheme\": \"https\",\n",
    "        }\n",
    "    )\n",
    "    cur = con.connect()\n",
    "    \n",
    "    output = {}\n",
    "    # Import trips\n",
    "    trip_sql = f\"\"\"\n",
    "    with agency as ( -- get agency id for each route\n",
    "        select distinct route_id, agency_id from mtadatalake.core.fact_gtfs_routes \n",
    "        where bundle = '{bundle}'\n",
    "        )\n",
    "    SELECT trips.route_id ,trip_id, service_id,trip_headsign,direction_id,block_id,shape_id,boarding_type,bundle, agency.agency_id\n",
    "    FROM mtadatalake.core.fact_gtfs_trips trips\n",
    "    join agency on trips.route_id = agency.route_id\n",
    "    where trips.bundle = '{bundle}'\n",
    "    \"\"\"\n",
    "    if import_trips == True:\n",
    "        print('Loading trips')\n",
    "        f = cur.execute(text(trip_sql))\n",
    "        trips_df = pd.DataFrame(f.fetchall())\n",
    "        output[\"trips\"] = trips_df\n",
    "\n",
    "    # Import shapes\n",
    "    shape_sql = f\"\"\"\n",
    "    SELECT shape_id, shape_pt_sequence, shape_pt_lat, shape_pt_lon, bundle\n",
    "    FROM mtadatalake.core.fact_gtfs_shapes\n",
    "    where bundle = '{bundle}'\n",
    "    \"\"\"\n",
    "    if import_shapes == True:\n",
    "        print(\"Loading shapes\")\n",
    "        f = cur.execute(text(shape_sql))\n",
    "        shapes_df = pd.DataFrame(f.fetchall())\n",
    "        output[\"shapes\"] = shapes_df\n",
    "\n",
    "    # Import stops\n",
    "    stop_sql = f\"\"\"\n",
    "    SELECT fact_gtfs_stops.stop_id, stop_name, stop_lat, stop_lon, shape_ref.revenue_stop, bundle\n",
    "    FROM mtadatalake.core.fact_gtfs_stops\n",
    "    left join (\n",
    "        SELECT stop_id, MAX(revenue_stop) AS revenue_stop\n",
    "        FROM mtadatalake.core.fact_gtfs_shape_reference\n",
    "        where bundle = '{bundle}'\n",
    "        GROUP BY stop_id\n",
    "    ) shape_ref\n",
    "    on fact_gtfs_stops.stop_id = shape_ref.stop_id\n",
    "    where bundle = '{bundle}'\n",
    "    \"\"\"\n",
    "    if import_stops == True:\n",
    "        print(\"Loading stops\")\n",
    "        f = cur.execute(text(stop_sql))\n",
    "        stops_df = pd.DataFrame(f.fetchall())\n",
    "        output[\"stops\"] = stops_df\n",
    "\n",
    "    # Import stop_times\n",
    "    stoptime_sql = f\"\"\"\n",
    "    SELECT trip_id, stop_id, arrival_time, departure_time, timepoint, stop_sequence, pickup_type, drop_off_type, bundle\n",
    "    FROM mtadatalake.core.fact_gtfs_stop_times\n",
    "    where bundle = '{bundle}'\n",
    "    \"\"\"\n",
    "    if import_stoptimes == True:\n",
    "        print(\"Loading stop times\")\n",
    "        f = cur.execute(text(stoptime_sql))\n",
    "        stoptimes_df = pd.DataFrame(f.fetchall())\n",
    "        output[\"stoptimes\"] = stoptimes_df\n",
    "\n",
    "    # import calendar\n",
    "    calendar_sql = f\"\"\" \n",
    "    SELECT service_id, monday, tuesday, wednesday, thursday, friday, saturday, sunday, start_date, end_date, bundle, modified_time, loaded_time\n",
    "    FROM mtadatalake.core.fact_gtfs_calendar\n",
    "    where bundle = '{bundle}'\n",
    "    \"\"\"\n",
    "    if import_calendar == True:\n",
    "        print(\"Loading calendar\")\n",
    "        f = cur.execute(text(calendar_sql))\n",
    "        calendar_df = pd.DataFrame(f.fetchall())\n",
    "        output[\"calendar\"] = calendar_df\n",
    "\n",
    "    #import calendar_dates\n",
    "    calendar_dates_sql = f\"\"\"\n",
    "    SELECT service_id, \"date\", exception_type\n",
    "    FROM mtadatalake.core.fact_gtfs_calendar_dates\n",
    "    where bundle = '{bundle}'\n",
    "    \"\"\"\n",
    "    if import_calendar_dates == True:\n",
    "        print(\"Loading calendar_dates\")\n",
    "        f = cur.execute(text(calendar_dates_sql))\n",
    "        calendar_dates_df = pd.DataFrame(f.fetchall())\n",
    "        output[\"calendar_dates\"] = calendar_dates_df\n",
    "\n",
    "    #import routes\n",
    "    routes_sql = f\"\"\"\n",
    "    SELECT route_id, route_short_name, route_long_name\n",
    "    FROM mtadatalake.core.fact_gtfs_routes\n",
    "    where bundle = '{bundle}'\n",
    "    \"\"\"\n",
    "    if import_calendar_dates == True:\n",
    "        print(\"Loading routes\")\n",
    "        f = cur.execute(text(routes_sql))\n",
    "        routes_df = pd.DataFrame(f.fetchall())\n",
    "        output[\"routes\"] = routes_df\n",
    "\n",
    "    \n",
    "    if cache_bundle == True:\n",
    "        with open(picklefile, \"wb\") as handle:\n",
    "            pickle.dump(output, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(f\"GTFS data for {bundle} successfully loaded and cached\")\n",
    "    else:\n",
    "       print(f\"GTFS data for {bundle} successfully loaded\") \n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3540d26d-03e2-49b2-abb0-aafb1564ff99",
   "metadata": {},
   "source": [
    "### Variant class with metrics as attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7728c9be-8b40-4940-94bb-f75096860bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variant: # abandon class, calculate outside\n",
    "    def __init__(self, shape_id, bundle):\n",
    "        self.shape_id = shape_id\n",
    "        self.bundle = bundle\n",
    "\n",
    "        ### GTFS \n",
    "        self.stops = self.bundle['stops']\n",
    "        self.stop_times = self.bundle['stoptimes']\n",
    "        self.trips = self.bundle['trips']\n",
    "        self.calendar = self.bundle['calendar']\n",
    "        self.calendar_dates = self.bundle['calendar_dates']\n",
    "        \n",
    "        \n",
    "\n",
    "        ### Metrics\n",
    "        self.trip_count = self._get_trip_count()\n",
    "        if self.trip_count == 0: # to filter out variants with no trips later\n",
    "            raise ValueError\n",
    "        self.stop_set = self._get_stop_set()\n",
    "        self.stop_count = self._get_stop_count()\n",
    "        self.headsign = self._get_headsign()\n",
    "        self.startend = self._get_start_end_stops()\n",
    "        self.length = self._get_shape_length()\n",
    "        self.direction = self._get_direction()\n",
    "        self.daytype = self._get_daytype()\n",
    "        self.school = self._get_runs_only_during_school_hours()\n",
    "\n",
    "    ### Metric 1 Getter ### \n",
    "    def _get_trip_count(self):\n",
    "        return self.trips[self.trips['shape_id'] == self.shape_id].shape[0] # turn into groupby, split into weekday, saturday, sunday \n",
    "\n",
    "    ### Metric 2 Getter ### \n",
    "    def _get_stop_set(self):\n",
    "        # Create mapping: trip_id â†’ shape_id\n",
    "        trip_to_shape = self.trips.set_index('trip_id')['shape_id'].to_dict()\n",
    "    \n",
    "        # Find first trip_id for this shape_id\n",
    "        shape_to_trip = {}\n",
    "        for trip_id, shape_id in trip_to_shape.items():\n",
    "            if shape_id not in shape_to_trip:\n",
    "                shape_to_trip[shape_id] = trip_id ## groupby using first\n",
    "    \n",
    "        shape_id = self.shape_id\n",
    "        if shape_id not in shape_to_trip:\n",
    "            return []\n",
    "    \n",
    "        trip_id = shape_to_trip[shape_id]\n",
    "    \n",
    "        # Get stop_times for this trip\n",
    "        grouped = self.stop_times.groupby('trip_id')\n",
    "        if trip_id not in grouped.groups:\n",
    "            return []\n",
    "    \n",
    "        trip_stops = grouped.get_group(trip_id).sort_values('stop_sequence')\n",
    "        if trip_stops.empty:\n",
    "            return []\n",
    "    \n",
    "        # Map stop_id â†’ stop_name\n",
    "        stop_id_to_name = self.stops.set_index('stop_id')['stop_name'].to_dict()\n",
    "    \n",
    "        # Ordered list of stop names\n",
    "        ordered_stop_names = [\n",
    "            stop_id_to_name.get(row['stop_id'], row['stop_id'])\n",
    "            for _, row in trip_stops.iterrows() ## avoid itterows\n",
    "        ]\n",
    "        return ordered_stop_names\n",
    "\n",
    "    ### Metric 3 ###\n",
    "    def _get_stop_count(self):\n",
    "        return len(self._get_stop_set()) ## can be done in metric 2\n",
    "\n",
    "        \n",
    "    ### Metric 4 Getter ###\n",
    "    def _get_headsign(self):\n",
    "        headsign_map = self.trips[['shape_id', 'trip_headsign']].drop_duplicates(subset='shape_id') ## \n",
    "        headsign_dict = dict(zip(headsign_map['shape_id'], headsign_map['trip_headsign']))\n",
    "        return headsign_dict.get(self.shape_id, None) ## can be done in metric 1 and 2\n",
    "    \n",
    "    ### Metric 5 Getter ### \n",
    "    def _get_start_end_stops(self):\n",
    "        stop_names = self._get_stop_set()\n",
    "        if not stop_names:\n",
    "            return (None, None)\n",
    "        return (stop_names[0], stop_names[-1])\n",
    "\n",
    "    ### Metric 6 Getter ###\n",
    "    def _get_shape_length(self):\n",
    "\n",
    "        # Filter stop_times for this shape_id\n",
    "        relevant_trips = self.trips[self.trips['shape_id'] == self.shape_id]\n",
    "        if relevant_trips.empty:\n",
    "            return None\n",
    "        relevant_stop_times = self.stop_times[self.stop_times['trip_id'].isin(relevant_trips['trip_id'])]\n",
    "\n",
    "        # Use the first trip as a representative to get stop sequence\n",
    "        first_trip_id = relevant_trips.iloc[0]['trip_id']\n",
    "        trip_stop_times = relevant_stop_times[relevant_stop_times['trip_id'] == first_trip_id]\n",
    "        trip_stop_times = trip_stop_times.sort_values(by='stop_sequence')\n",
    "\n",
    "        # Merge with stops to get lat/lon\n",
    "        stops_with_coords = trip_stop_times.merge(self.stops, on='stop_id', how='left')\n",
    "\n",
    "        # Check if we have enough valid points\n",
    "        if stops_with_coords[['stop_lat', 'stop_lon']].isnull().any().any():\n",
    "            return np.nan  # Incomplete stop coordinates\n",
    "        \n",
    "        # Create a LineString from the ordered stop points\n",
    "        line = LineString(zip(stops_with_coords['stop_lon'], stops_with_coords['stop_lat']))\n",
    "\n",
    "        # Convert to GeoSeries and set CRS \n",
    "        geo_line = gpd.GeoSeries([line], crs=\"EPSG:4326\")\n",
    "\n",
    "        # Reproject to another CRS to measure in feet\n",
    "        geo_line_feet = geo_line.to_crs(\"EPSG:2263\")\n",
    "\n",
    "        # Return length\n",
    "        return geo_line_feet.length.iloc[0]\n",
    "        \n",
    "    ### Metric 6 ### \n",
    "    def _get_direction(self):\n",
    "        directions = self.trips[self.trips['shape_id'] == self.shape_id]['direction_id'].drop_duplicates()\n",
    "        if directions.empty:\n",
    "            return None\n",
    "        return directions.iloc[0] ## can be done in first merge\n",
    "\n",
    "\n",
    "    ### Metric 7 ### \n",
    "    def _get_daytype(self):\n",
    "        shape_trips = self.trips[self.trips['shape_id'] == self.shape_id]\n",
    "        service_ids = shape_trips['service_id'].unique()\n",
    "        service_days = self.calendar[self.calendar['service_id'].isin(service_ids)]\n",
    "    \n",
    "        # List of day columns\n",
    "        day_columns = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "    \n",
    "        # Sum across all service_ids to find which daytypes are served\n",
    "        active_days = (service_days[day_columns].sum() > 0)\n",
    "    \n",
    "        # Get list of active day names\n",
    "        active_day_names = active_days[active_days].index.tolist()\n",
    "    \n",
    "        # Decide daytype based on active days\n",
    "        weekdays = {'monday', 'tuesday', 'wednesday', 'thursday', 'friday'}\n",
    "        weekends = {'saturday', 'sunday'}\n",
    "    \n",
    "        active_day_set = set(active_day_names)\n",
    "    \n",
    "        if active_day_set.issubset(weekdays):\n",
    "            return 'weekday'\n",
    "        elif active_day_set.issubset(weekends):\n",
    "            return 'weekend'\n",
    "        else: ### to explore: is weekend being undercounted?\n",
    "            return 'weekday' ## take out because trips already filtered\n",
    "    ### Metric 8 ### \n",
    "    def _get_runs_only_during_school_hours(self):\n",
    "        morning_start = time(7, 0)\n",
    "        morning_end = time(9, 0)\n",
    "        afternoon_start = time(14, 0)\n",
    "        afternoon_end = time(16, 0)\n",
    "    \n",
    "        variant_trips = self.trips[self.trips['shape_id'] == self.shape_id]\n",
    "    \n",
    "        if variant_trips.empty:\n",
    "            return False\n",
    "    \n",
    "        first_stop_times = (\n",
    "            self.stop_times[\n",
    "                (self.stop_times['trip_id'].isin(variant_trips['trip_id'])) &\n",
    "                (self.stop_times['stop_sequence'] == 1)\n",
    "            ][['trip_id', 'departure_time']]\n",
    "        )\n",
    "    \n",
    "        def parse_time(t_str):\n",
    "            h, m, s = map(int, t_str.split(':'))\n",
    "            h = h % 24  # Normalize 24+ hour times\n",
    "            return time(h, m, s)\n",
    "    \n",
    "        first_stop_times['dep_time_obj'] = first_stop_times['departure_time'].apply(parse_time)\n",
    "    \n",
    "        def in_school_hours(t):\n",
    "            return (morning_start <= t <= morning_end) or (afternoon_start <= t <= afternoon_end)\n",
    "    \n",
    "        # Check if ALL trips are within school hours\n",
    "        all_in_school = first_stop_times['dep_time_obj'].apply(in_school_hours).all()\n",
    "    \n",
    "        return all_in_school\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ca1b51-9cac-47eb-858f-c2ec683f31fb",
   "metadata": {},
   "source": [
    "# Variant List\n",
    "#### Creates a list of all variants for all routes of any bundle. Gives variants an 'inferred variant type' by comparing it to the 'main' variant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e57398-29d4-4bb3-9be4-db8df68b96a8",
   "metadata": {},
   "source": [
    "### Step 0: Loading in Bundles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17bcec87-2517-4208-9a3a-20003e6a68f6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trips\n",
      "Loading shapes\n",
      "Loading stops\n",
      "Loading stop times\n",
      "Loading calendar\n",
      "Loading calendar_dates\n",
      "Loading routes\n",
      "GTFS data for 2024Jan_Prod_r01_b03_Predate_Shuttles_v2_i1_scheduled successfully loaded\n",
      "Loading trips\n",
      "Loading shapes\n",
      "Loading stops\n",
      "Loading stop times\n",
      "Loading calendar\n",
      "Loading calendar_dates\n",
      "Loading routes\n",
      "GTFS data for 2024April_Prod_r01_b07_Predate_02_Shuttles_2_SCHEDULED successfully loaded\n"
     ]
    }
   ],
   "source": [
    "bundle_names = [\n",
    "    '2024Jan_Prod_r01_b03_Predate_Shuttles_v2_i1_scheduled',\n",
    "    '2024April_Prod_r01_b07_Predate_02_Shuttles_2_SCHEDULED'\n",
    "]\n",
    "bundle_dfs = [import_bundle(name) for name in bundle_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba6a1b3-2e77-4580-9c64-8b1dd7d9e697",
   "metadata": {},
   "source": [
    "#### Step 0a: Filter to most representative days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3061e65-2977-4724-bfcb-b7f9d55b5901",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Takes in a bundle dictionary, along with the dates and days of week you'd like to filter to, \n",
    "#### returns the filtered dataframes as the new bundle dict\n",
    "def bundlefilter(bundle, target_dates, days_of_week):\n",
    "    trips = bundle['trips']\n",
    "    calendar = bundle['calendar']\n",
    "    calendar_dates = bundle['calendar_dates']\n",
    "    routes = bundle['routes']\n",
    "\n",
    "    filtered_trips = pd.DataFrame()\n",
    "\n",
    "    for date, day in zip(target_dates, days_of_week):\n",
    "        # Base services active on the day\n",
    "        base_services = calendar[\n",
    "            (calendar[day] == 1) &\n",
    "            (calendar['start_date'] <= date) &\n",
    "            (calendar['end_date'] >= date)\n",
    "        ]['service_id']\n",
    "\n",
    "        # Exceptions on that date\n",
    "        exceptions = calendar_dates[calendar_dates['date'] == date]\n",
    "        removed = exceptions[exceptions['exception_type'] == 2]['service_id']\n",
    "        added = exceptions[exceptions['exception_type'] == 1]['service_id']\n",
    "\n",
    "        # Apply exception logic\n",
    "        final_services = pd.concat([\n",
    "            base_services[~base_services.isin(removed)],\n",
    "            added\n",
    "        ]).drop_duplicates()\n",
    "\n",
    "        # Filter trips for this date\n",
    "        trips_today = trips[trips['service_id'].isin(final_services)]\n",
    "        filtered_trips = pd.concat([filtered_trips, trips_today])\n",
    "\n",
    "    # Remove duplicates\n",
    "    filtered_trips = filtered_trips.drop_duplicates()\n",
    "\n",
    "    # Filter stop_times to match trips\n",
    "    stop_times = bundle['stoptimes']\n",
    "    filtered_stop_times = stop_times[stop_times['trip_id'].isin(filtered_trips['trip_id'])]\n",
    "\n",
    "    # Return a new filtered bundle\n",
    "    return {\n",
    "        'trips': filtered_trips.reset_index(drop=True),\n",
    "        'stoptimes': filtered_stop_times.reset_index(drop=True),\n",
    "        'stops': bundle['stops'], \n",
    "        'shapes': bundle['shapes'],  \n",
    "        'calendar': calendar,\n",
    "        'calendar_dates': calendar_dates,\n",
    "        'routes': routes\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f468f6d7-42bc-4f5a-89e9-dde8669caf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter bundles\n",
    "bundle1 = bundlefilter(bundle_dfs[0],\n",
    "                                target_dates=[20240108, 20240113, 20240107],\n",
    "                                days_of_week=['monday', 'saturday', 'sunday'])\n",
    "\n",
    "bundle2 = bundlefilter(bundle_dfs[1],\n",
    "                                target_dates=[20240402, 20240406, 20240407],\n",
    "                                days_of_week=['tuesday', 'saturday', 'sunday'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97b2fbd-fabf-443a-b076-3c26a7ac3bf5",
   "metadata": {},
   "source": [
    "#### Step 1: Creating variant objects, average loading time of 1 minute per route "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2aa227f8-e0ca-4f88-90c2-c165871f8fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Takes in trips df and route id and returns unique shape ids tied to that route\n",
    "def find_variants(trips, route_id):\n",
    "    shape_ids = trips[trips['route_id'] == route_id]['shape_id'].dropna().unique()\n",
    "    return list(shape_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cbfb47-cc8b-4c7c-bc52-722ed357f0ec",
   "metadata": {},
   "source": [
    "##### Step 1a: Load in all routes, filter if quicker output is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7efa75aa-75c8-4549-b396-0b16a0c25405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n### for speed, filered to select queens routes\\nqueens_routes = [\\n    r for r in all_routes \\n    if str(r).startswith('Q') and str(r)[-1].isdigit() and int(str(r)[-1]) % 3 == 0\\n]\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "all_routes = bundle1['trips']['route_id'].unique()\n",
    "\n",
    "'''\n",
    "### for speed, filered to select queens routes\n",
    "queens_routes = [\n",
    "    r for r in all_routes \n",
    "    if str(r).startswith('Q') and str(r)[-1].isdigit() and int(str(r)[-1]) % 3 == 0\n",
    "]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4500a39-4880-4034-8120-4a2b0517bd06",
   "metadata": {},
   "source": [
    "##### Step 1b: Create a nested dictionary: route_id â†’ { shape_id â†’ Variant object }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee02295e-e818-4547-94ef-2271746da2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_variants = {}\n",
    "\n",
    "### for speed, filter 'all_routes'\n",
    "for route in all_routes:\n",
    "    variants_for_route = find_variants(bundle1['trips'], route)\n",
    "    # create all Variant objects for this route at once\n",
    "    variants = {}\n",
    "    for variant in variants_for_route:\n",
    "        try:\n",
    "            v = Variant(variant, bundle1)\n",
    "            variants[variant] = v\n",
    "        except ValueError:\n",
    "            # Skip shapes with zero trips\n",
    "            continue\n",
    "    all_variants[route] = variants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc634b1a-616d-4e16-8a54-7bffadea00c6",
   "metadata": {},
   "source": [
    "##### Step 2: Define some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff94d86d-f972-4c62-bb5d-a7036defb8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### takes in a variant dictionary (where the keys are the shape id strs and the values are variant objects)\n",
    "### returns a group dictionary (where the key is the daytype-direction and the values are variant objects of that daytype-direction) \n",
    "def group_variants(variants):\n",
    "    group = defaultdict(list)\n",
    "    for v in variants.values():  \n",
    "        # Create a single string key for easy reading\n",
    "        key = f\"{v.daytype}_{v.direction}\"\n",
    "        group[key].append(v)\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5cf9b49-b324-4bb1-90ca-302007fc3fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### identifies the 'main' variant of a dictionary of variants via a weighted average score of trip_count, length, and stop_count\n",
    "def find_main_variant(variants):\n",
    "    trip_weight = 0.6\n",
    "    length_weight = 0.3\n",
    "    stop_weight = 0.1\n",
    "\n",
    "    def score(v):\n",
    "        length = v.length if v.length is not None else 0\n",
    "        return (trip_weight * v.trip_count) + (length_weight * length / 1000) + (stop_weight * v.stop_count)\n",
    "\n",
    "    return max(variants, key=score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "641c4eaa-4adc-4c5e-8f47-a49edb319ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Takes in a variant and its corresponding 'main' variant to compare it to\n",
    "### returns its inferred variant type\n",
    "def inferred_variant_type(variant, main):\n",
    "    if variant.shape_id == main.shape_id:\n",
    "        return \"main\"\n",
    "    elif variant.trip_count <= 2: ## If only 1 or 2 trips, classify as other.\n",
    "        return \"other/unknown\"\n",
    "    elif ((main.stop_count - variant.stop_count)/main.stop_count > 0.4) or any(word in variant.headsign.lower() for word in (\"limited\",\"ltd\")): \n",
    "        ## if 'limited' or 'express' in headsign, or if both stop count is at least 40 percent smaller than main\n",
    "        return \"limited\"\n",
    "    elif (variant.daytype == 'weekday') and (variant.school == True):\n",
    "        ## if on weekdays and durings school hours\n",
    "        return \"school\"\n",
    "    elif ((main.stop_count - variant.stop_count)/main.stop_count) > 0.15 and ((main.length - variant.length)/main.length) > 0.15:\n",
    "        ## if both stop count and length are more than 15 percent shorter/smaller than main\n",
    "        return \"short\"\n",
    "    else:\n",
    "        return \"branch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d79041fd-fcc2-48e6-b22b-3e7fcd1a960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in variants dictionary where the keys are the shape id strs and values are the variant objects\n",
    "# returns an output table of variant, inferred variant type, daytype-direction, and all metrics\n",
    "def build_variant_table(variants):\n",
    "    grouped = group_variants(variants)\n",
    "    rows = []\n",
    "    \n",
    "    for group_key, group in grouped.items():\n",
    "        main_variant = find_main_variant(group)\n",
    "        for v in group:\n",
    "            variant_type = inferred_variant_type(v, main_variant)\n",
    "            rows.append({\n",
    "                \"shape_id\": v.shape_id,\n",
    "                \"daytype_direction\": group_key,\n",
    "                \"inferred_variant_type\": variant_type,\n",
    "                \"trip_count\": v.trip_count,\n",
    "                \"stop_count\": v.stop_count,\n",
    "                \"length (in ft)\": v.length,\n",
    "                \"headsign\": v.headsign,\n",
    "                \"start_end_stops\": v.startend,\n",
    "            })\n",
    "\n",
    "    \n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50a940e8-2750-49b0-8dd5-57d97b6e68ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in larger variant dictionary where the keys are route id strs, and the values are variant dictionaries\n",
    "# Using build_variant_table, builds a larger output table with all routes\n",
    "def build_all_routes_variant_table(all_variants_dict):\n",
    "    dfs = []\n",
    "    for route_id, variants_dict in all_variants_dict.items():\n",
    "        df = build_variant_table(variants_dict) \n",
    "        df['route_id'] = route_id  # add route id column\n",
    "        dfs.append(df)\n",
    "    all_variants_df = pd.concat(dfs, ignore_index=True)\n",
    "    return all_variants_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca05f91-ab1d-484c-9686-36cc3c92390e",
   "metadata": {},
   "source": [
    "##### Step 3: Build and export to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b75aabe4-37d0-4998-80f0-8253e8cfdcac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  shape_id daytype_direction inferred_variant_type  trip_count  stop_count  \\\n",
      "0  S480018         weekday_1                  main         221          46   \n",
      "1  S480021         weekday_1         other/unknown           2          17   \n",
      "2  S480010         weekday_1         other/unknown           1          24   \n",
      "3  S480019         weekday_1                 short           5          32   \n",
      "4  S480020         weekday_1         other/unknown           1          27   \n",
      "\n",
      "   length (in ft)                         headsign  \\\n",
      "0    33983.135557  ARLINGTON HOLLAND AV via FOREST   \n",
      "1    11434.160780  ARLINGTON HOLLAND AV via FOREST   \n",
      "2    17077.121742                      RICHMOND AV   \n",
      "3    24229.762306                      RICHMOND AV   \n",
      "4    19314.595620  ARLINGTON HOLLAND AV via FOREST   \n",
      "\n",
      "                                     start_end_stops route_id  \n",
      "0  (ST GEORGE FERRY/RAMP C S48 & S98, HOLLAND AV/...      S48  \n",
      "1  (WILLOWBROOK RD/HOUSTON ST, HOLLAND AV/BENJAMI...      S48  \n",
      "2       (FOREST AV/OXFORD PL, FOREST AV/RICHMOND AV)      S48  \n",
      "3  (ST GEORGE FERRY/RAMP C S48 & S98, FOREST AV/R...      S48  \n",
      "4       (FOREST AV/BROADWAY, HOLLAND AV/BENJAMIN PL)      S48  \n"
     ]
    }
   ],
   "source": [
    "final_df = build_all_routes_variant_table(all_variants)\n",
    "final_df.to_excel(\"all_routes_variants.xlsx\", index=False)\n",
    "print(final_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ff8776-5421-46af-a3a4-1843d78f57ff",
   "metadata": {},
   "source": [
    "##### Step 4: Define similarity metrics for variant to variant comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a5bb38-5b98-424e-84c8-4787ed9e1887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers: similarity metrics\n",
    "def stop_set_similarity(s1, s2):\n",
    "    if not s1 and not s2:\n",
    "        return 1.0\n",
    "    if not s1 or not s2:\n",
    "        return 0.0\n",
    "    inter = len(s1 & s2)\n",
    "    union = len(s1 | s2)\n",
    "    return inter / union if union else 0.0\n",
    "\n",
    "def shape_similarity(sh1,sh2)\n",
    "\n",
    "def start_end_match(a, b):\n",
    "    return 1.0 if (a and b and a[0] == b[0] and a[1] == b[1]) else 0.0\n",
    "\n",
    "def headsign_match(a, b):\n",
    "    if a is None or b is None:\n",
    "        return 0.0\n",
    "    return 1.0 if a.strip().lower() == b.strip().lower() else 0.0\n",
    "\n",
    "def ratio_score(a, b):\n",
    "    # returns value in [0,1] closeness of ratio to 1\n",
    "    if a is None or b is None or (a == 0 and b == 0):\n",
    "        return 0.0\n",
    "    r = min(a,b) / max(a,b) if max(a,b) else 0.0\n",
    "    return r\n",
    "\n",
    "# composite similarity (weights can be changed)\n",
    "def variant_similarity(v1, v2, weights=None):\n",
    "    # v1, v2 are dict-like with keys used below\n",
    "    if weights is None:\n",
    "        weights = {\n",
    "            'stop_set_similarity': 0.35,\n",
    "            'startend': 0.20,\n",
    "            'headsign': 0.10,\n",
    "            'trip_ratio': 0.10,\n",
    "            'length_ratio': 0.15,\n",
    "            'stopcount_ratio': 0.10\n",
    "        }\n",
    "    sc = {}\n",
    "    sc['stop_set_similarity'] = stop_set_similarity(v1['stop_id_set'], v2['stop_id_set'])\n",
    "    sc['startend'] = start_end_match(v1.get('startend'), v2.get('startend'))\n",
    "    sc['headsign'] = headsign_match(v1.get('headsign'), v2.get('headsign'))\n",
    "    sc['trip_ratio'] = ratio_score(v1.get('trip_count',0), v2.get('trip_count',0))\n",
    "    sc['length_ratio'] = ratio_score(v1.get('length',0), v2.get('length',0))\n",
    "    sc['stopcount_ratio'] = ratio_score(v1.get('stop_count',0), v2.get('stop_count',0))\n",
    "\n",
    "    total = 0.0\n",
    "    for k,w in weights.items():\n",
    "        total += w * sc[k]\n",
    "    return total, sc  # returns composite and breakdown"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17089a41-3338-4af7-a301-b2af25dd98e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, shutil, zipfile, csv\n",
    "import requests\n",
    "import psycopg2\n",
    "import fiona\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas\n",
    "from sqlalchemy import URL, create_engine, text\n",
    "from trino.auth import OAuth2Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d89f2d9f-6719-4a2c-9b1c-a544fd694f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in parent directories to sys.path to get multiHook library\n",
    "current_dir = os.getcwd()\n",
    "for x in range(4):  # Look four levels up\n",
    "    parent_dir = os.path.dirname(current_dir)\n",
    "    if parent_dir not in sys.path:\n",
    "        sys.path.append(parent_dir)\n",
    "    current_dir = parent_dir\n",
    "\n",
    "import multihook.pycnxn.dbhook as dbhook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c37734d7-7a53-4c51-9aff-23bc91822a39",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def import_bundle(\n",
    "    bundle,\n",
    "    import_trips=True,\n",
    "    import_shapes=True,\n",
    "    import_stops=True,\n",
    "    import_stoptimes=True,\n",
    "    import_calendar=True,\n",
    "    import_calendar_dates=True,\n",
    "    import_routes=True,\n",
    "    cache_bundle=False,\n",
    "):\n",
    "    \"\"\"Import GTFS data from the following tables in ADLS:\n",
    "    - core.dbo.fact_gtfs_trips\n",
    "    - core.dbo.fact_gtfs_shapes\n",
    "    - core.dbo.fact_gtfs_stops\n",
    "    - core.dbo.fact_gtfs_shape_reference\n",
    "    - core.dbo.fact_gtfs_stop_times\n",
    "\n",
    "    - ADD IN CALENDAR, CALENDAR DATES, AND ROUTES\n",
    "    \"\"\"\n",
    "\n",
    "    # GTFS bundle can take some time to import. If there is already a cached GTFS bundle pickle file in the repository, read in that data instead\n",
    "    picklefile = bundle + \".pickle\"\n",
    "    if os.path.isfile(picklefile) == True:\n",
    "        with open(picklefile, \"rb\") as handle:\n",
    "            gtfs_bundle = pickle.load(handle)\n",
    "            return gtfs_bundle\n",
    "\n",
    "    con = create_engine(\n",
    "        r\"trino://trino-route-trino.apps.mtasiprod.eastus.aroapp.io:443/mtadatalake\",\n",
    "        connect_args={\n",
    "            \"auth\": OAuth2Authentication(),\n",
    "            \"http_scheme\": \"https\",\n",
    "        }\n",
    "    )\n",
    "    cur = con.connect()\n",
    "    \n",
    "    output = {}\n",
    "    # Import trips\n",
    "    trip_sql = f\"\"\"\n",
    "    with agency as ( -- get agency id for each route\n",
    "        select distinct route_id, agency_id from mtadatalake.core.fact_gtfs_routes \n",
    "        where bundle = '{bundle}'\n",
    "        )\n",
    "    SELECT trips.route_id ,trip_id, service_id,trip_headsign,direction_id,block_id,shape_id,boarding_type,bundle, agency.agency_id\n",
    "    FROM mtadatalake.core.fact_gtfs_trips trips\n",
    "    join agency on trips.route_id = agency.route_id\n",
    "    where trips.bundle = '{bundle}'\n",
    "    \"\"\"\n",
    "    if import_trips == True:\n",
    "        print('Loading trips')\n",
    "        f = cur.execute(text(trip_sql))\n",
    "        trips_df = pd.DataFrame(f.fetchall())\n",
    "        output[\"trips\"] = trips_df\n",
    "\n",
    "    # Import shapes\n",
    "    shape_sql = f\"\"\"\n",
    "    SELECT shape_id, shape_pt_sequence, shape_pt_lat, shape_pt_lon, bundle\n",
    "    FROM mtadatalake.core.fact_gtfs_shapes\n",
    "    where bundle = '{bundle}'\n",
    "    \"\"\"\n",
    "    if import_shapes == True:\n",
    "        print(\"Loading shapes\")\n",
    "        f = cur.execute(text(shape_sql))\n",
    "        shapes_df = pd.DataFrame(f.fetchall())\n",
    "        output[\"shapes\"] = shapes_df\n",
    "\n",
    "    # Import stops\n",
    "    stop_sql = f\"\"\"\n",
    "    SELECT fact_gtfs_stops.stop_id, stop_name, stop_lat, stop_lon, shape_ref.revenue_stop, bundle\n",
    "    FROM mtadatalake.core.fact_gtfs_stops\n",
    "    left join (\n",
    "        SELECT stop_id, MAX(revenue_stop) AS revenue_stop\n",
    "        FROM mtadatalake.core.fact_gtfs_shape_reference\n",
    "        where bundle = '{bundle}'\n",
    "        GROUP BY stop_id\n",
    "    ) shape_ref\n",
    "    on fact_gtfs_stops.stop_id = shape_ref.stop_id\n",
    "    where bundle = '{bundle}'\n",
    "    \"\"\"\n",
    "    if import_stops == True:\n",
    "        print(\"Loading stops\")\n",
    "        f = cur.execute(text(stop_sql))\n",
    "        stops_df = pd.DataFrame(f.fetchall())\n",
    "        output[\"stops\"] = stops_df\n",
    "\n",
    "    # Import stop_times\n",
    "    stoptime_sql = f\"\"\"\n",
    "    SELECT trip_id, stop_id, arrival_time, departure_time, timepoint, stop_sequence, pickup_type, drop_off_type, bundle\n",
    "    FROM mtadatalake.core.fact_gtfs_stop_times\n",
    "    where bundle = '{bundle}'\n",
    "    \"\"\"\n",
    "    if import_stoptimes == True:\n",
    "        print(\"Loading stop times\")\n",
    "        f = cur.execute(text(stoptime_sql))\n",
    "        stoptimes_df = pd.DataFrame(f.fetchall())\n",
    "        output[\"stoptimes\"] = stoptimes_df\n",
    "\n",
    "    # import calendar\n",
    "    calendar_sql = f\"\"\" \n",
    "    SELECT service_id, monday, tuesday, wednesday, thursday, friday, saturday, sunday, start_date, end_date, bundle, modified_time, loaded_time\n",
    "    FROM mtadatalake.core.fact_gtfs_calendar\n",
    "    where bundle = '{bundle}'\n",
    "    \"\"\"\n",
    "    if import_calendar == True:\n",
    "        print(\"Loading calendar\")\n",
    "        f = cur.execute(text(calendar_sql))\n",
    "        calendar_df = pd.DataFrame(f.fetchall())\n",
    "        output[\"calendar\"] = calendar_df\n",
    "\n",
    "    #import calendar_dates\n",
    "    calendar_dates_sql = f\"\"\"\n",
    "    SELECT service_id, \"date\", exception_type\n",
    "    FROM mtadatalake.core.fact_gtfs_calendar_dates\n",
    "    where bundle = '{bundle}'\n",
    "    \"\"\"\n",
    "    if import_calendar_dates == True:\n",
    "        print(\"Loading calendar_dates\")\n",
    "        f = cur.execute(text(calendar_dates_sql))\n",
    "        calendar_dates_df = pd.DataFrame(f.fetchall())\n",
    "        output[\"calendar_dates\"] = calendar_dates_df\n",
    "\n",
    "    #import routes\n",
    "    routes_sql = f\"\"\"\n",
    "    SELECT route_id, route_short_name, route_long_name\n",
    "    FROM mtadatalake.core.fact_gtfs_routes\n",
    "    where bundle = '{bundle}'\n",
    "    \"\"\"\n",
    "    if import_calendar_dates == True:\n",
    "        print(\"Loading routes\")\n",
    "        f = cur.execute(text(routes_sql))\n",
    "        routes_df = pd.DataFrame(f.fetchall())\n",
    "        output[\"routes\"] = routes_df\n",
    "\n",
    "    \n",
    "    if cache_bundle == True:\n",
    "        with open(picklefile, \"wb\") as handle:\n",
    "            pickle.dump(output, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(f\"GTFS data for {bundle} successfully loaded and cached\")\n",
    "    else:\n",
    "       print(f\"GTFS data for {bundle} successfully loaded\") \n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f029923c-b8d6-4f79-a1ca-f6a694bf5a9b",
   "metadata": {},
   "source": [
    "### Generate Summary for each Route of Specified Bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9450e4be-9acf-4ef9-846b-3c2f5318a491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trips\n",
      "Loading shapes\n",
      "Loading stops\n",
      "Loading stop times\n",
      "Loading calendar\n",
      "Loading calendar_dates\n",
      "Loading routes\n",
      "GTFS data for 2025June_Prod_r04_b01_PREDATE_SHUTTLES_SCHEDULED successfully loaded\n"
     ]
    }
   ],
   "source": [
    "gtfs = import_bundle('2025June_Prod_r04_b01_PREDATE_SHUTTLES_SCHEDULED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1ea9633f-c5ad-4c71-be08-4b74c091770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = gtfs['trips']\n",
    "stoptimes = gtfs['stoptimes']\n",
    "stops = gtfs['stops']\n",
    "shapes = gtfs['shapes']\n",
    "calendar = gtfs['calendar']\n",
    "calendar_dates = gtfs['calendar_dates']\n",
    "routes = gtfs['routes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "effb4df8-990d-4118-bbfe-ba3790ef05c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_period(mins):\n",
    "    if 0 <= mins < 240:       # 12:00 AM – 4:00 AM\n",
    "        return 'Overnight'\n",
    "    elif 240 <= mins < 360:   # 4:00 AM – 6:00 AM\n",
    "        return 'Early Morning'\n",
    "    elif 360 <= mins < 540:   # 6:00 AM – 9:00 AM\n",
    "        return 'AM'\n",
    "    elif 540 <= mins < 900:   # 9:00 AM – 3:00 PM\n",
    "        return 'Midday'\n",
    "    elif 900 <= mins < 1140:  # 3:00 PM – 7:00 PM\n",
    "        return 'PM'\n",
    "    elif 1140 <= mins < 1260: # 7:00 PM – 9:00 PM\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Late Evening'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "98428b91-7690-4dfe-8373-7f764907ae03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set target date\n",
    "target_date = 20250702\n",
    "target_day = 'wednesday'\n",
    "\n",
    "# Get active service_ids on that date\n",
    "base_services = calendar[\n",
    "    (calendar[target_day] == 1) &\n",
    "    (calendar['start_date'] <= target_date) &\n",
    "    (calendar['end_date'] >= target_date)\n",
    "]['service_id']\n",
    "\n",
    "exceptions = calendar_dates[calendar_dates['date'] == target_date]\n",
    "removed = exceptions[exceptions['exception_type'] == 2]['service_id']\n",
    "added = exceptions[exceptions['exception_type'] == 1]['service_id']\n",
    "\n",
    "final_services = pd.concat([base_services[~base_services.isin(removed)], added]).drop_duplicates()\n",
    "\n",
    "### Get valid trips for that date\n",
    "valid_trips = trips[trips['service_id'].isin(final_services)]\n",
    "\n",
    "#Join with routes to get route_short_name\n",
    "valid_trips= valid_trips.merge(routes[['route_id', 'route_long_name']], on='route_id', how='left')\n",
    "\n",
    "# Get the first stop per trip\n",
    "stoptimes['stop_sequence'] = stoptimes['stop_sequence'].astype(int)\n",
    "first_stops = stoptimes.sort_values(['trip_id', 'stop_sequence']).groupby('trip_id').first().reset_index()\n",
    "# Only keep trip_id and arrival_time\n",
    "first_stops = first_stops[['trip_id', 'arrival_time']] \n",
    "# Now join with trips to get route_id and shape_id\n",
    "trip_times = valid_trips[['trip_id', 'route_id','route_long_name','shape_id','direction_id']].merge(first_stops, on='trip_id')\n",
    "\n",
    "\n",
    "# Convert 'arrival_time' to total minutes since midnight\n",
    "split_time = trip_times['arrival_time'].str.split(':', expand=True).astype(int)\n",
    "trip_times['minutes'] = split_time[0] * 60 + split_time[1]\n",
    "\n",
    "\n",
    "# filter out the ones that go into the next day\n",
    "trip_times = trip_times[trip_times['minutes'] < 1440]\n",
    "\n",
    "\n",
    "# Group and summarize \n",
    "trip_times['period'] = trip_times['minutes'].apply(label_period)\n",
    "\n",
    "      \n",
    "summary = trip_times.groupby(['route_id','route_long_name', 'direction_id', 'period']).agg(\n",
    "    trips=('trip_id', 'count')\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "# Pivot to wide format\n",
    "pivot_summary = summary.pivot_table(index=['route_id', 'route_long_name','direction_id'], columns='period', values='trips', fill_value=0).astype(int)\n",
    "\n",
    "# Add total trips and variants\n",
    "pivot_summary['Total Trips'] = trip_times.groupby(['route_id', 'route_long_name', 'direction_id'])['trip_id'].count()\n",
    "pivot_summary['Total Variants'] = trip_times.groupby(['route_id','route_long_name', 'direction_id'])['shape_id'].nunique()\n",
    "\n",
    "\n",
    "# Compute frequencies (number of trips divided by hours in range )\n",
    "overnight = pivot_summary['Overnight'].replace(0,pd.NA).round(2) / 4\n",
    "early = pivot_summary['Early Morning'].replace(0, pd.NA).round(2) / 2\n",
    "am = pivot_summary['AM'].replace(0, pd.NA).round(2) / 3\n",
    "midday = pivot_summary['Midday'].replace(0, pd.NA).round(2) / 6 \n",
    "pm =  pivot_summary['PM'].replace(0, pd.NA).round(2) / 4\n",
    "evening =  pivot_summary['Evening'].replace(0, pd.NA).round(2) / 2\n",
    "late =  pivot_summary['Late Evening'].replace(0, pd.NA).round(2) / 3 \n",
    "\n",
    "\n",
    "\n",
    "# Replace in DataFrame\n",
    "pivot_summary['Overnight'] = pd.to_numeric(overnight,errors = 'coerce').round(1)\n",
    "pivot_summary['Early Morning'] = pd.to_numeric(early,errors = 'coerce').round(1)\n",
    "pivot_summary['AM'] = pd.to_numeric(am, errors='coerce').round(1)\n",
    "pivot_summary['Midday'] = pd.to_numeric(midday, errors='coerce').round(1)\n",
    "pivot_summary['PM'] = pd.to_numeric(pm,errors='coerce').round(1)\n",
    "pivot_summary['Evening'] = pd.to_numeric(evening,errors='coerce').round(1)\n",
    "pivot_summary['Late Evening'] = pd.to_numeric(late,errors = 'coerce').round(1)\n",
    "\n",
    "# Reorder columns\n",
    "order = ['Overnight','Early Morning','AM', 'Midday', 'PM', 'Evening','Late Evening', 'Total Trips', 'Total Variants']\n",
    "pivot_summary1 = pivot_summary[order]\n",
    "\n",
    "\n",
    "collapsed = pivot_summary1.groupby(['route_id', 'route_long_name']).agg({\n",
    "    'Overnight': 'max',\n",
    "    'Early Morning': 'max',\n",
    "    'AM': 'max',\n",
    "    'Midday': 'max',\n",
    "    'PM': 'max',\n",
    "    'Evening': 'max',\n",
    "    'Late Evening': 'max',\n",
    "    'Total Trips': 'sum',\n",
    "    'Total Variants': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "\n",
    "collapsed = collapsed[['route_id', 'route_long_name'] + order]\n",
    "\n",
    "\n",
    "collapsed.to_csv('route_summary_frequencies1.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5f1e2e8f-6da3-4461-9995-84ab2a3fc0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trips\n",
      "Loading shapes\n",
      "Loading stops\n",
      "Loading stop times\n",
      "Loading calendar\n",
      "Loading calendar_dates\n",
      "Loading routes\n",
      "GTFS data for 2025March_Prod_r01_b05_SHUTTLES_PREDATE_SCHEDULED successfully loaded\n"
     ]
    }
   ],
   "source": [
    "gtfs = import_bundle('2025March_Prod_r01_b05_SHUTTLES_PREDATE_SCHEDULED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d53aab8f-21b8-4b78-a48d-6ebaa4ceb1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = gtfs['trips']\n",
    "stoptimes = gtfs['stoptimes']\n",
    "stops = gtfs['stops']\n",
    "shapes = gtfs['shapes']\n",
    "calendar = gtfs['calendar']\n",
    "calendar_dates = gtfs['calendar_dates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fdacf8a3-dc1e-4d31-8939-93854dae240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set target date\n",
    "target_date = 20250401\n",
    "target_day = 'tuesday'\n",
    "\n",
    "# Get active service_ids on that date\n",
    "base_services = calendar[\n",
    "    (calendar[target_day] == 1) &\n",
    "    (calendar['start_date'] <= target_date) &\n",
    "    (calendar['end_date'] >= target_date)\n",
    "]['service_id']\n",
    "\n",
    "exceptions = calendar_dates[calendar_dates['date'] == target_date]\n",
    "removed = exceptions[exceptions['exception_type'] == 2]['service_id']\n",
    "added = exceptions[exceptions['exception_type'] == 1]['service_id']\n",
    "\n",
    "final_services = pd.concat([base_services[~base_services.isin(removed)], added]).drop_duplicates()\n",
    "\n",
    "### Get valid trips for that date\n",
    "valid_trips = trips[trips['service_id'].isin(final_services)]\n",
    "\n",
    "#Join with routes to get route_short_name\n",
    "valid_trips= valid_trips.merge(routes[['route_id', 'route_short_name']], on='route_id', how='left')\n",
    "\n",
    "# Get the first stop per trip\n",
    "stoptimes['stop_sequence'] = stoptimes['stop_sequence'].astype(int)\n",
    "first_stops = stoptimes.sort_values(['trip_id', 'stop_sequence']).groupby('trip_id').first().reset_index()\n",
    "# Only keep trip_id and arrival_time\n",
    "first_stops = first_stops[['trip_id', 'arrival_time']] \n",
    "# Now join with trips to get route_id and shape_id\n",
    "trip_times = valid_trips[['trip_id', 'route_id','route_short_name','shape_id','direction_id']].merge(first_stops, on='trip_id')\n",
    "\n",
    "\n",
    "# Convert 'arrival_time' to total minutes since midnight\n",
    "split_time = trip_times['arrival_time'].str.split(':', expand=True).astype(int)\n",
    "trip_times['minutes'] = split_time[0] * 60 + split_time[1]\n",
    "\n",
    "\n",
    "# filter out the ones that go into the next day\n",
    "trip_times = trip_times[trip_times['minutes'] < 1440]\n",
    "\n",
    "\n",
    "# Group and summarize \n",
    "trip_times['period'] = trip_times['minutes'].apply(label_period)\n",
    "\n",
    "      \n",
    "summary = trip_times.groupby(['route_id','route_short_name', 'direction_id', 'period']).agg(\n",
    "    trips=('trip_id', 'count')\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "# Pivot to wide format\n",
    "pivot_summary = summary.pivot_table(index=['route_id', 'route_short_name','direction_id'], columns='period', values='trips', fill_value=0).astype(int)\n",
    "\n",
    "# Add total trips and variants\n",
    "pivot_summary['Total Trips'] = trip_times.groupby(['route_id', 'route_short_name', 'direction_id'])['trip_id'].count()\n",
    "pivot_summary['Total Variants'] = trip_times.groupby(['route_id','route_short_name', 'direction_id'])['shape_id'].nunique()\n",
    "\n",
    "\n",
    "# Compute frequencies (number of trips divided by hours in range )\n",
    "overnight = pivot_summary['Overnight'].replace(0,pd.NA).round(2) / 4\n",
    "early = pivot_summary['Early Morning'].replace(0, pd.NA).round(2) / 2\n",
    "am = pivot_summary['AM'].replace(0, pd.NA).round(2) / 3\n",
    "midday = pivot_summary['Midday'].replace(0, pd.NA).round(2) / 6 \n",
    "pm =  pivot_summary['PM'].replace(0, pd.NA).round(2) / 4\n",
    "evening =  pivot_summary['Evening'].replace(0, pd.NA).round(2) / 2\n",
    "late =  pivot_summary['Late Evening'].replace(0, pd.NA).round(2) / 3 \n",
    "\n",
    "\n",
    "# Replace in DataFrame\n",
    "pivot_summary['Overnight'] = pd.to_numeric(overnight,errors = 'coerce').round(1)\n",
    "pivot_summary['Early Morning'] = pd.to_numeric(early,errors = 'coerce').round(1)\n",
    "pivot_summary['AM'] = pd.to_numeric(am, errors='coerce').round(1)\n",
    "pivot_summary['Midday'] = pd.to_numeric(midday, errors='coerce').round(1)\n",
    "pivot_summary['PM'] = pd.to_numeric(pm,errors='coerce').round(1)\n",
    "pivot_summary['Evening'] = pd.to_numeric(evening,errors='coerce').round(1)\n",
    "pivot_summary['Late Evening'] = pd.to_numeric(late,errors = 'coerce').round(1)\n",
    "\n",
    "\n",
    "# Reorder columns\n",
    "order = ['Overnight','Early Morning','AM', 'Midday', 'PM', 'Evening','Late Evening', 'Total Trips', 'Total Variants']\n",
    "pivot_summary1 = pivot_summary[order]\n",
    "\n",
    "collapsed = pivot_summary1.groupby(['route_id','route_short_name']).agg({\n",
    "    'Overnight': 'max',\n",
    "    'Early Morning': 'max',\n",
    "    'AM': 'max',\n",
    "    'Midday': 'max',\n",
    "    'PM': 'max',\n",
    "    'Evening': 'max',\n",
    "    'Late Evening': 'max',\n",
    "    'Total Trips': 'sum',\n",
    "    'Total Variants': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "\n",
    "collapsed = collapsed[['route_id','route_short_name'] + order] \n",
    "\n",
    "collapsed.to_csv('route_summary_frequencies2.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c012edd4-b5d4-46ce-ad91-906ece72a5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('route_summary_frequencies1.csv')\n",
    "df2 = pd.read_csv('route_summary_frequencies2.csv')\n",
    "# Rename columns for clarity before merge\n",
    "\n",
    "df1.columns = [f\"{col}_b1\" if col not in ['route_id','route_long_name', 'Total Trips','Total Variants'] else col for col in df1.columns]\n",
    "df2.columns = [f\"{col}_b2\" if col not in ['route_id', 'hour'] else col for col in df2.columns]\n",
    "\n",
    "# Merge on route_id, direction_id\n",
    "merged = pd.merge(\n",
    "    df1,\n",
    "    df2,\n",
    "    on=['route_id',],\n",
    "    how='outer',\n",
    "    sort=True\n",
    ")\n",
    "\n",
    "merged.to_csv(\"route_summary_frequencies_combined1.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109e25be-8b20-43dd-ac98-c38498cae279",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

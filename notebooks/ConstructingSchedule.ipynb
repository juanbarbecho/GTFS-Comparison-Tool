{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39631783-8ab5-4ee8-b33a-bbacc271e6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, shutil, zipfile, csv\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas\n",
    "from sqlalchemy import URL, create_engine, text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103367fe-f00a-42ff-926d-cd6d022f889f",
   "metadata": {},
   "source": [
    "# Read in GTFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9353eaac-1d93-4dfa-beb7-9eda4d3a8cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_bundle(\n",
    "    bundle,\n",
    "    import_trips=True,\n",
    "    import_shapes=True,\n",
    "    import_stops=True,\n",
    "    import_stoptimes=True,\n",
    "    import_calendar=True,\n",
    "    import_calendar_dates=True,\n",
    "    cache_bundle=False,\n",
    "):\n",
    "    \"\"\"Import GTFS data from the following tables in ADLS:\n",
    "    - core.dbo.fact_gtfs_trips\n",
    "    - core.dbo.fact_gtfs_shapes\n",
    "    - core.dbo.fact_gtfs_stops\n",
    "    - core.dbo.fact_gtfs_shape_reference\n",
    "    - core.dbo.fact_gtfs_stop_times\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # GTFS bundle can take some time to import. If there is already a cached GTFS bundle pickle file in the repository, read in that data instead\n",
    "    picklefile = bundle + \".pickle\"\n",
    "    if os.path.isfile(picklefile) == True:\n",
    "        with open(picklefile, \"rb\") as handle:\n",
    "            gtfs_bundle = pickle.load(handle)\n",
    "            return gtfs_bundle\n",
    "\n",
    "    con = create_engine(\n",
    "        r\"trino://trino-route-trino.apps.mtasiprod.eastus.aroapp.io:443/mtadatalake\",\n",
    "        connect_args={\n",
    "            \"auth\": OAuth2Authentication(),\n",
    "            \"http_scheme\": \"https\",\n",
    "        }\n",
    "    )\n",
    "    cur = con.connect()\n",
    "    \n",
    "    output = {}\n",
    "    # Import trips\n",
    "    trip_sql = f\"\"\"\n",
    "    with agency as ( -- get agency id for each route\n",
    "        select distinct route_id, agency_id from mtadatalake.core.fact_gtfs_routes \n",
    "        where bundle = '{bundle}'\n",
    "        )\n",
    "    SELECT trips.route_id ,trip_id, service_id,trip_headsign,direction_id,block_id,shape_id,boarding_type,bundle, agency.agency_id\n",
    "    FROM mtadatalake.core.fact_gtfs_trips trips\n",
    "    join agency on trips.route_id = agency.route_id\n",
    "    where trips.bundle = '{bundle}'\n",
    "    \"\"\"\n",
    "    if import_trips == True:\n",
    "        print('Loading trips')\n",
    "        f = cur.execute(text(trip_sql))\n",
    "        trips_df = pd.DataFrame(f.fetchall())\n",
    "        output[\"trips\"] = trips_df\n",
    "\n",
    "    # Import shapes\n",
    "    shape_sql = f\"\"\"\n",
    "    SELECT shape_id, shape_pt_sequence, shape_pt_lat, shape_pt_lon, bundle\n",
    "    FROM mtadatalake.core.fact_gtfs_shapes\n",
    "    where bundle = '{bundle}'\n",
    "    \"\"\"\n",
    "    if import_shapes == True:\n",
    "        print(\"Loading shapes\")\n",
    "        f = cur.execute(text(shape_sql))\n",
    "        shapes_df = pd.DataFrame(f.fetchall())\n",
    "        output[\"shapes\"] = shapes_df\n",
    "\n",
    "    # Import stops\n",
    "    stop_sql = f\"\"\"\n",
    "    SELECT fact_gtfs_stops.stop_id, stop_name, stop_lat, stop_lon, shape_ref.revenue_stop, bundle\n",
    "    FROM mtadatalake.core.fact_gtfs_stops\n",
    "    left join (\n",
    "        SELECT stop_id, MAX(revenue_stop) AS revenue_stop\n",
    "        FROM mtadatalake.core.fact_gtfs_shape_reference\n",
    "        where bundle = '{bundle}'\n",
    "        GROUP BY stop_id\n",
    "    ) shape_ref\n",
    "    on fact_gtfs_stops.stop_id = shape_ref.stop_id\n",
    "    where bundle = '{bundle}'\n",
    "    \"\"\"\n",
    "    if import_stops == True:\n",
    "        print(\"Loading stops\")\n",
    "        f = cur.execute(text(stop_sql))\n",
    "        stops_df = pd.DataFrame(f.fetchall())\n",
    "        output[\"stops\"] = stops_df\n",
    "\n",
    "    # Import stop_times\n",
    "    stoptime_sql = f\"\"\"\n",
    "    SELECT trip_id, stop_id, arrival_time, departure_time, timepoint, stop_sequence, pickup_type, drop_off_type, bundle\n",
    "    FROM mtadatalake.core.fact_gtfs_stop_times\n",
    "    where bundle = '{bundle}'\n",
    "    \"\"\"\n",
    "    if import_stoptimes == True:\n",
    "        print(\"Loading stop times\")\n",
    "        f = cur.execute(text(stoptime_sql))\n",
    "        stoptimes_df = pd.DataFrame(f.fetchall())\n",
    "        output[\"stoptimes\"] = stoptimes_df\n",
    "\n",
    "    # import calendar\n",
    "    calendar_sql = f\"\"\" \n",
    "    SELECT service_id, monday, tuesday, wednesday, thursday, friday, saturday, sunday, start_date, end_date, bundle, modified_time, loaded_time\n",
    "    FROM mtadatalake.core.fact_gtfs_calendar\n",
    "    where bundle = '{bundle}'\n",
    "    \"\"\"\n",
    "    if import_calendar == True:\n",
    "        print(\"Loading calendar\")\n",
    "        f = cur.execute(text(calendar_sql))\n",
    "        calendar_df = pd.DataFrame(f.fetchall())\n",
    "        output[\"calendar\"] = calendar_df\n",
    "\n",
    "    #import calendar_dates\n",
    "    calendar_dates_sql = f\"\"\"\n",
    "    SELECT service_id, \"date\", exception_type\n",
    "    FROM mtadatalake.core.fact_gtfs_calendar_dates\n",
    "    where bundle = '{bundle}'\n",
    "    \"\"\"\n",
    "    if import_calendar_dates == True:\n",
    "        print(\"Loading calendar_dates\")\n",
    "        f = cur.execute(text(calendar_dates_sql))\n",
    "        calendar_dates_df = pd.DataFrame(f.fetchall())\n",
    "        output[\"calendar_dates\"] = calendar_dates_df\n",
    "\n",
    "    \n",
    "    if cache_bundle == True:\n",
    "        with open(picklefile, \"wb\") as handle:\n",
    "            pickle.dump(output, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(f\"GTFS data for {bundle} successfully loaded and cached\")\n",
    "    else:\n",
    "       print(f\"GTFS data for {bundle} successfully loaded\") \n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d27f336-1002-4bbf-9060-2f85f5340108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_representative(bundle1,bundle2):\n",
    "    trino = dbhook.Hook(adls_trino)\n",
    "\n",
    "    representative_day_query = f'''\n",
    "    with bundle_dates AS (-- Get a list of dates and bundles\n",
    "      SELECT service_date, format_datetime(service_date, 'EEEE') day_of_week, bundle, pick_year, pick_name, sched_type, manual\n",
    "      FROM mtadatalake.core.dim_bus_gtfs_bundle_dates\n",
    "      where bundle in ('{bundle1}', '{bundle2}')\n",
    "      --  where pick_year in (2024) \n",
    "    )\n",
    "    , gtfs_calendar_fixed_dates as (\n",
    "      -- Correct invalid dates like February 31st\n",
    "      select service_id, monday, tuesday, wednesday, thursday, friday, saturday, sunday, bundle\n",
    "        , start_date/10000 start_yr, start_date/100 % 100 start_mnth\n",
    "        -- Take out dates like Feb 31 or Apr 31\n",
    "        , case \n",
    "          when start_date/100 % 100 = 2 and start_date % 100 > 28 then 28 \n",
    "          when (start_date/10000) % 4 = 0 and start_date % 100 > 29 then 29 -- if it's a leap year, 2/29\n",
    "          when start_date/100 % 100 in (9, 4, 6, 11) and start_date % 100 > 30 then 30\n",
    "          else start_date % 100\n",
    "        end as start_day\n",
    "        , end_date/10000 end_yr, end_date/100 % 100 end_mnth\n",
    "        , case\n",
    "          when end_date/100 % 100 = 2 and end_date % 100 > 28 then 28\n",
    "          when (end_date/10000) % 4 = 0 and end_date % 100 > 29 then 29 -- if it's a leap year, 2/29\n",
    "          when end_date/100 % 100 in (9, 4, 6, 11) and end_date % 100 > 30 then 30\n",
    "          else end_date % 100\n",
    "        end as end_day\n",
    "      from mtadatalake.core.fact_gtfs_calendar\n",
    "      where bundle in (select distinct bundle from bundle_dates)\n",
    "    )\n",
    "    , calendar as (\n",
    "      -- convert start_date to date object\n",
    "      select service_id, monday, tuesday, wednesday, thursday, friday, saturday, sunday, bundle\n",
    "        , DATE(cast(start_yr AS VARCHAR) || '-' || CAST(start_mnth AS VARCHAR) || '-' || CAST(start_day AS VARCHAR)) as start_date\n",
    "        , DATE(CAST(end_yr AS VARCHAR) || '-' || CAST(end_mnth AS VARCHAR) || '-' || CAST(end_day AS VARCHAR)) AS end_date\n",
    "      from gtfs_calendar_fixed_dates\n",
    "    )\n",
    "    , exceptions as (\n",
    "      SELECT service_id, \"date\", exception_type, bundle\n",
    "        , DATE(cast((\"date\" / 10000) as varchar) || '-' || cast(((\"date\" / 100) % 100) as varchar) || '-' || cast((\"date\" % 100) as varchar)) AS service_date\n",
    "        , format_datetime(DATE(cast((\"date\" / 10000) as varchar) || '-' || cast(((\"date\" / 100) % 100) as varchar) || '-' || cast((\"date\" % 100) as varchar)), 'EEEE') as day_of_week\n",
    "      FROM mtadatalake.core.fact_gtfs_calendar_dates\n",
    "      where bundle in (select distinct bundle from bundle_dates)\n",
    "    )\n",
    "    , base_schedule as (\n",
    "      -- for each day, assemble a row for each service id serving it\n",
    "      SELECT bd.service_date, bd.day_of_week, c.service_id, c.bundle, e.exception_type\n",
    "      FROM bundle_dates bd\n",
    "      inner join calendar c\n",
    "        ON bd.bundle = c.bundle \n",
    "        and bd.service_date BETWEEN c.start_date AND c.end_date\n",
    "        AND (\n",
    "          (bd.day_of_week = 'Monday' AND c.monday = 1) OR\n",
    "          (bd.day_of_week = 'Tuesday' AND c.tuesday = 1) OR\n",
    "          (bd.day_of_week = 'Wednesday' AND c.wednesday = 1) OR\n",
    "          (bd.day_of_week = 'Thursday' AND c.thursday = 1) OR\n",
    "          (bd.day_of_week = 'Friday' AND c.friday = 1) OR\n",
    "          (bd.day_of_week = 'Saturday' AND c.saturday = 1) OR\n",
    "          (bd.day_of_week = 'Sunday' AND c.sunday = 1)\n",
    "          )\n",
    "      -- Join in dates and service_ids where service was removed (exception_type 2)\n",
    "      left join (select * from exceptions where exception_type = 2) e\n",
    "        on c.bundle = e.bundle\n",
    "        and bd.service_date = e.service_date\n",
    "        and c.service_id = e.service_id\n",
    "    )\n",
    "    , modified_schedules as (\n",
    "      -- Join in dates and service_ids where service was added (exception_type 1)\n",
    "      select service_date, day_of_week, service_id, bundle, exception_type\n",
    "      from base_schedule \n",
    "      where exception_type is null or exception_type = 1\n",
    "      union all\n",
    "      select service_date, day_of_week, service_id, bundle, exception_type\n",
    "      from exceptions \n",
    "      where exception_type = 1\n",
    "    )\n",
    "    , daily_schedules as (\n",
    "      -- Designate schedule daytype\n",
    "      select service_date, day_of_week, service_id, bundle\n",
    "        , CASE \n",
    "            WHEN day_of_week(service_date) BETWEEN 1 AND 5 THEN 'Weekday'\n",
    "            WHEN day_of_week(service_date) = 6 THEN 'Saturday'\n",
    "            WHEN day_of_week(service_date) = 7 THEN 'Sunday'\n",
    "            ELSE null\n",
    "          end as sched_daytype\n",
    "      from modified_schedules\n",
    "    )\n",
    "    , schedules_per_day AS (\n",
    "        -- Aggregate service_ids for each day to form a \"schedule\"\n",
    "        SELECT \n",
    "            bundle, service_date, sched_daytype\n",
    "            , array_join(array_agg(CAST(service_id AS VARCHAR) ORDER BY service_id), ',') AS schedule\n",
    "        FROM daily_schedules\n",
    "        GROUP BY bundle, service_date, sched_daytype\n",
    "    )\n",
    "    , schedule_variations AS (\n",
    "      -- Count the occurrences of each unique schedule\n",
    "      SELECT bundle, service_date, schedule, sched_daytype\n",
    "        , COUNT(service_date) over (partition by bundle, schedule, sched_daytype) AS sched_var_frequency\n",
    "        -- Make a name for each unique schedule, e.g. Weekday-1, Saturday-3\n",
    "        ,  sched_daytype || '-' || CAST(DENSE_RANK() OVER (PARTITION BY bundle, sched_daytype ORDER BY schedule) AS VARCHAR) AS schedule_variation\n",
    "      FROM schedules_per_day\n",
    "    )\n",
    "    , schedule_variations_ranked as ( -- this CTE returns one row per day, with the schedule variation and rank\n",
    "      select bundle, service_date, schedule, sched_daytype, schedule_variation, sched_var_frequency\n",
    "        , row_number() over (partition by bundle, sched_daytype order by sched_var_frequency desc, service_date) ranking_by_freq\n",
    "      from schedule_variations\n",
    "    )\n",
    "    , most_representative_day as ( -- this CTE will give a weekday, saturday, and sunday date for each bundle\n",
    "      -- When ranking_by_freq = 1, that day is the best representation of a weekday, saturday, or sunday schedule\n",
    "        select bundle, service_date, sched_daytype as day_of_week\n",
    "        from schedule_variations_ranked\n",
    "        where ranking_by_freq = 1\n",
    "    )\n",
    "    -- decide between schedule_variations_ranked, most_representative_day, or sched_with_service_id\n",
    "    select * from most_representative_day\n",
    "    '''\n",
    "    return trino.frame(representative_day_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1583306c-b150-4ea1-aa58-aaec7fd8aa53",
   "metadata": {},
   "source": [
    "### Loading in Historical GTFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e83eb8c6-8fd7-4322-b96a-c40692f1b482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trips\n",
      "Loading shapes\n",
      "Loading stops\n",
      "Loading stop times\n",
      "Loading calendar\n",
      "Loading calendar_dates\n",
      "GTFS data for 2025June_Prod_r04_b01_PREDATE_SHUTTLES_SCHEDULED successfully loaded\n"
     ]
    }
   ],
   "source": [
    "gtfshistorical = import_bundle('2025June_Prod_r04_b01_PREDATE_SHUTTLES_SCHEDULED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "93d19596-26e6-4f0b-ad35-9cdf62b1321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_h = gtfshistorical['trips']\n",
    "stoptimes_h = gtfshistorical['stoptimes']\n",
    "stops_h = gtfshistorical['stops']\n",
    "calendar_h = gtfshistorical['calendar']\n",
    "calendar_dates_h = gtfshistorical['calendar_dates']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e920d0ba-8786-4d74-88ad-1fb6bebdf5e2",
   "metadata": {},
   "source": [
    "### Constructing schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "23a90d9b-f0a6-4de7-bc4e-343676c1352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define date, day_of_week\n",
    "targetdates = [20250702,20250705,20250706]\n",
    "days_of_week = ['wednesday','saturday','sunday']\n",
    "schedules = []\n",
    "#looping through each date\n",
    "for i in range(3):\n",
    "    ### get service_ids from fact_gtfs_calendar\n",
    "    ## where service_date is between start and end, and day of week being the same\n",
    "    base_service_ids = calendar_h[(calendar_h[days_of_week[i]] == 1) & (calendar_h['start_date'] <= targetdates[i]) & (calendar_h['end_date'] >= targetdates[i])]['service_id']\n",
    "    # Join in service_ids where service was removed (exception_type 2) from calendar_dates\n",
    "    # Join in service_ids where service was added (exception_type 1) from calendar_dates\n",
    "    exceptions = calendar_dates_h[calendar_dates_h['date'] == targetdates[i]]\n",
    "    removed = exceptions[exceptions['exception_type'] == 2]['service_id']\n",
    "    added = exceptions[exceptions['exception_type'] == 1]['service_id']\n",
    "    \n",
    "    #not including type 2\n",
    "    baseminusremoved = base_service_ids[~base_service_ids.isin(removed)]\n",
    "    \n",
    "    #including type 1\n",
    "    finalservices = pd.concat([baseminusremoved,added]).drop_duplicates()\n",
    "\n",
    "    # for each service_id, get trips associated\n",
    "    # finalservices becomes a df\n",
    "    tripstoday = trips_h.merge(finalservices.to_frame(name='service_id'), on='service_id', how='inner')\n",
    "\n",
    "    # for each of those trips, get stop_times \n",
    "    stoptimestoday = stoptimes_h.merge(tripstoday[['route_id','direction_id','trip_id']], on='trip_id', how='inner')\n",
    "    \n",
    "    ### Calculating Average (or Median) Running Time and Frequency\n",
    "    # creating hour and minute manually from arrival_time\n",
    "    stoptimestoday[['hour', 'minute','seconds']] = stoptimestoday['arrival_time'].str.split(':', expand=True).astype(int)\n",
    "    stoptimestoday['time_in_minutes'] = stoptimestoday['hour'] * 60 + stoptimestoday['minute']\n",
    "\n",
    "    ## creating origin and destination\n",
    "    stoptimestoday['stop_sequence'] = stoptimestoday['stop_sequence'].astype(int)\n",
    "    \n",
    "    # Group by trip_id\n",
    "    groupedbytrip = stoptimestoday.groupby('trip_id')['stop_sequence']\n",
    "\n",
    "    # Origin = 1 if stop_sequence is the minimum in the trip\n",
    "    stoptimestoday['origin'] = (stoptimestoday['stop_sequence'] == groupedbytrip.transform('min')).astype(int)\n",
    "\n",
    "    # Destination = 1 if stop_sequence is the maximum in the trip\n",
    "    stoptimestoday['destination'] = (stoptimestoday['stop_sequence'] == groupedbytrip.transform('max')).astype(int)\n",
    "    stoptimestoday['hour'] = stoptimestoday['hour'] % 24\n",
    "\n",
    "    # Filtering to only starts and ends\n",
    "    STTfiltered = stoptimestoday[((stoptimestoday['origin']==1)| (stoptimestoday['destination'] ==1))]\n",
    "\n",
    "    # Get start time from origin rows\n",
    "    origin = STTfiltered[STTfiltered['origin'] == 1][['trip_id', 'route_id','direction_id','hour','time_in_minutes']]\n",
    "    origin = origin.rename(columns={'time_in_minutes': 'start_min'})\n",
    "    \n",
    "    # Get end time from destination rows\n",
    "    dest = STTfiltered[STTfiltered['destination'] == 1][['trip_id', 'time_in_minutes']]\n",
    "    dest = dest.rename(columns={'time_in_minutes': 'end_min'})\n",
    "    \n",
    "    # Merge origin and destination rows on trip_id\n",
    "    trip_summary = pd.merge(origin, dest, on='trip_id')\n",
    "    \n",
    "    # Duration in minutes\n",
    "    trip_summary['duration_min'] = trip_summary['end_min'] - trip_summary['start_min']\n",
    "\n",
    "    # Remove trips that start before 0 or end at or after 1440\n",
    "    trip_summary = trip_summary[(trip_summary['start_min'] >= 0) & (trip_summary['end_min'] < 1440)]\n",
    "\n",
    "    # Final summary: group by route and hour\n",
    "    result = trip_summary.groupby(['route_id','direction_id','hour']).agg(\n",
    "        frequency=('trip_id', 'count'),\n",
    "        avg_running_time=('duration_min', 'median')\n",
    "    ).reset_index().sort_values(['route_id','direction_id', 'hour'])\n",
    "\n",
    "    # Flagging those that have considerable difference between the mean and median\n",
    "    threshold = 1\n",
    "    result['Flag'] = abs(result['avg_running_time'] - trip_summary.groupby(['route_id','direction_id','hour'])['duration_min'].mean().values) > threshold\n",
    "\n",
    "    # Create a complete grid of route_id, direction_id, and hours 0-23\n",
    "    all_hours = pd.DataFrame({'hour': list(range(24))})\n",
    "    routes = trip_summary[['route_id', 'direction_id']].drop_duplicates()\n",
    "    full_grid = routes.merge(all_hours, how='cross')\n",
    "    \n",
    "    # Merge result with full_grid to ensure every hour appears\n",
    "    result = full_grid.merge(result, on=['route_id', 'direction_id', 'hour'], how='left')\n",
    "    \n",
    "    # Fill Day and Date for all rows\n",
    "    result['Day'] = days_of_week[i]\n",
    "    result['Date'] = targetdates[i]\n",
    "    result = result.sort_values(by=['route_id', 'direction_id', 'hour'])\n",
    "    \n",
    "    schedules.append(result)\n",
    "\n",
    "finalschedule1 = pd.concat(schedules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9c39bce2-d38e-46d2-8d36-fc4fd97cae5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trips\n",
      "Loading shapes\n",
      "Loading stops\n",
      "Loading stop times\n",
      "Loading calendar\n",
      "Loading calendar_dates\n",
      "GTFS data for 2025March_Prod_r01_b05_SHUTTLES_PREDATE_SCHEDULED successfully loaded\n"
     ]
    }
   ],
   "source": [
    "gtfshistorical = import_bundle('2025March_Prod_r01_b05_SHUTTLES_PREDATE_SCHEDULED')\n",
    "trips_h = gtfshistorical['trips']\n",
    "stoptimes_h = gtfshistorical['stoptimes']\n",
    "stops_h = gtfshistorical['stops']\n",
    "calendar_h = gtfshistorical['calendar']\n",
    "calendar_dates_h = gtfshistorical['calendar_dates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0c25b39d-a314-47e2-930e-6fcdfdd6c4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define date, day_of_week\n",
    "targetdates = [20250401,20250405,20250406]\n",
    "days_of_week = ['tuesday','saturday','sunday']\n",
    "schedules = []\n",
    "#looping through each date\n",
    "for i in range(3):\n",
    "    ### get service_ids from fact_gtfs_calendar\n",
    "    ## where service_date is between start and end, and day of week being the same\n",
    "    base_service_ids = calendar_h[(calendar_h[days_of_week[i]] == 1) & (calendar_h['start_date'] <= targetdates[i]) & (calendar_h['end_date'] >= targetdates[i])]['service_id']\n",
    "    # Join in service_ids where service was removed (exception_type 2) from calendar_dates\n",
    "    # Join in service_ids where service was added (exception_type 1) from calendar_dates\n",
    "    exceptions = calendar_dates_h[calendar_dates_h['date'] == targetdates[i]]\n",
    "    removed = exceptions[exceptions['exception_type'] == 2]['service_id']\n",
    "    added = exceptions[exceptions['exception_type'] == 1]['service_id']\n",
    "    \n",
    "    #not including type 2\n",
    "    baseminusremoved = base_service_ids[~base_service_ids.isin(removed)]\n",
    "    \n",
    "    #including type 1\n",
    "    finalservices = pd.concat([baseminusremoved,added]).drop_duplicates()\n",
    "\n",
    "    # for each service_id, get trips associated (via merge)\n",
    "    # finalservices becomes a df\n",
    "    tripstoday = trips_h.merge(finalservices.to_frame(name='service_id'), on='service_id', how='inner')\n",
    "\n",
    "    # for each of those trips, get stop_times (via merge)\n",
    "    stoptimestoday = stoptimes_h.merge(tripstoday[['route_id','direction_id','trip_id']], on='trip_id', how='inner')\n",
    "    \n",
    "    ### Calculating Average (or Median) Running Time and Frequency\n",
    "    # creating hour and minute manually from arrival_time\n",
    "    stoptimestoday[['hour', 'minute','seconds']] = stoptimestoday['arrival_time'].str.split(':', expand=True).astype(int)\n",
    "    stoptimestoday['time_in_minutes'] = stoptimestoday['hour'] * 60 + stoptimestoday['minute']\n",
    "\n",
    "    ## creating origin and destination\n",
    "    stoptimestoday['stop_sequence'] = stoptimestoday['stop_sequence'].astype(int)\n",
    "    \n",
    "    # Group by trip_id\n",
    "    groupedbytrip = stoptimestoday.groupby('trip_id')['stop_sequence']\n",
    "\n",
    "    # Origin = 1 if stop_sequence is the minimum in the trip\n",
    "    stoptimestoday['origin'] = (stoptimestoday['stop_sequence'] == groupedbytrip.transform('min')).astype(int)\n",
    "\n",
    "    # Destination = 1 if stop_sequence is the maximum in the trip\n",
    "    stoptimestoday['destination'] = (stoptimestoday['stop_sequence'] == groupedbytrip.transform('max')).astype(int)\n",
    "    stoptimestoday['hour'] = stoptimestoday['hour'] % 24\n",
    "\n",
    "    # Filtering to only starts and ends\n",
    "    STTfiltered = stoptimestoday[((stoptimestoday['origin']==1)| (stoptimestoday['destination'] ==1))]\n",
    "\n",
    "    # Get start time from origin rows\n",
    "    origin = STTfiltered[STTfiltered['origin'] == 1][['trip_id', 'route_id','direction_id','hour','time_in_minutes']]\n",
    "    origin = origin.rename(columns={'time_in_minutes': 'start_min'})\n",
    "    \n",
    "    # Get end time from destination rows\n",
    "    dest = STTfiltered[STTfiltered['destination'] == 1][['trip_id', 'time_in_minutes']]\n",
    "    dest = dest.rename(columns={'time_in_minutes': 'end_min'})\n",
    "    \n",
    "    # Merge origin and destination rows on trip_id\n",
    "    trip_summary = pd.merge(origin, dest, on='trip_id')\n",
    "    \n",
    "    # Duration in minutes\n",
    "    trip_summary['duration_min'] = trip_summary['end_min'] - trip_summary['start_min']\n",
    "\n",
    "    # Remove trips that start before 0 or end at or after 1440\n",
    "    trip_summary = trip_summary[(trip_summary['start_min'] >= 0) & (trip_summary['end_min'] < 1440)]\n",
    "\n",
    "    # Final summary: group by route and hour\n",
    "    result = trip_summary.groupby(['route_id','direction_id','hour']).agg(\n",
    "        frequency=('trip_id', 'count'),\n",
    "        avg_running_time=('duration_min', 'median')\n",
    "    ).reset_index().sort_values(['route_id','direction_id', 'hour'])\n",
    "\n",
    "    # Flagging those that have considerable difference between the mean and median\n",
    "    threshold = 1\n",
    "    result['Flag'] = abs(result['avg_running_time'] - trip_summary.groupby(['route_id','direction_id','hour'])['duration_min'].mean().values) > threshold\n",
    "\n",
    "    # Create a complete grid of route_id, direction_id, and hours 0-23\n",
    "    all_hours = pd.DataFrame({'hour': list(range(24))})\n",
    "    routes = trip_summary[['route_id', 'direction_id']].drop_duplicates()\n",
    "    full_grid = routes.merge(all_hours, how='cross')\n",
    "    \n",
    "    # Merge result with full_grid to ensure every hour appears\n",
    "    result = full_grid.merge(result, on=['route_id', 'direction_id', 'hour'], how='left')\n",
    "    \n",
    "    # Fill Day and Date for all rows\n",
    "    result['Day'] = days_of_week[i]\n",
    "    result['Date'] = targetdates[i]\n",
    "    result = result.sort_values(by=['route_id', 'direction_id', 'hour'])\n",
    "    \n",
    "    schedules.append(result)\n",
    "\n",
    "finalschedule2 = pd.concat(schedules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "23d7d868-1ec8-49aa-b6cc-9b74b0d3eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(\n",
    "    finalschedule1,\n",
    "    finalschedule2,\n",
    "    on=[\"route_id\", \"direction_id\", \"hour\", \"Day\"],\n",
    "    how=\"outer\"\n",
    ")\n",
    "merged = merged.sort_values(by=[\"Day\",\"route_id\", \"direction_id\", \"hour\"])\n",
    "\n",
    "# Move 'Day' to the front\n",
    "cols = merged.columns.tolist()\n",
    "cols.remove(\"Day\")\n",
    "cols = [\"Day\"] + cols\n",
    "merged = merged[cols]\n",
    "\n",
    "\n",
    "# Save the merged result\n",
    "merged.to_csv(\"final_schedule.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39631783-8ab5-4ee8-b33a-bbacc271e6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, shutil, zipfile, csv\n",
    "import requests\n",
    "import psycopg2\n",
    "import fiona\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas\n",
    "from sqlalchemy import URL, create_engine, text\n",
    "from trino.auth import OAuth2Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6857ff82-68a7-4097-91b7-b04c0a1f98f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in parent directories to sys.path to get multiHook library\n",
    "current_dir = os.getcwd()\n",
    "for x in range(4):  # Look four levels up\n",
    "    parent_dir = os.path.dirname(current_dir)\n",
    "    if parent_dir not in sys.path:\n",
    "        sys.path.append(parent_dir)\n",
    "    current_dir = parent_dir\n",
    "\n",
    "import multihook.pycnxn.dbhook as dbhook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103367fe-f00a-42ff-926d-cd6d022f889f",
   "metadata": {},
   "source": [
    "# Read in GTFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7b1abaf-06f0-4559-ba75-bcc015970df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gtfs_zip(\n",
    "    gtfs_dir,\n",
    "    import_calendar=True,\n",
    "    import_routes=True,\n",
    "    import_shapes=True,\n",
    "    import_stops=True,\n",
    "    import_stoptimes=True,\n",
    "    import_trips=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Read gtfs files to pandas dataframes. Used when the input gtfs is a zip file\n",
    "    \"\"\"\n",
    "    \n",
    "    output = {}\n",
    "\n",
    "    # Import calendar\n",
    "    if import_calendar == True:\n",
    "        print(\"Loading calendar and calendar_dates\")\n",
    "        calendar_df = pd.read_csv(os.path.join(gtfs_dir, \"calendar.txt\"), dtype={\n",
    "            'service_id': 'str',  \n",
    "            'monday': 'bool',  \n",
    "            'tuesday': 'bool',  \n",
    "            'wednesday': 'bool',  \n",
    "            'thursday': 'bool',  \n",
    "            'friday': 'bool', \n",
    "            'saturday': 'bool',  \n",
    "            'sunday': 'bool',  \n",
    "            'start_date': 'str', \n",
    "            'end_date': 'str',\n",
    "        })\n",
    "        output['calendar'] = calendar_df\n",
    "    \n",
    "    # Import calendar_dates\n",
    "    if import_calendar == True:    \n",
    "        calendar_dates_df = pd.read_csv(os.path.join(gtfs_dir, \"calendar_dates.txt\"), dtype={\n",
    "            'service_id': 'str',  \n",
    "            'date': 'str',\n",
    "            'exception_type': 'Int64',\n",
    "        })\n",
    "        output['calendar_dates'] = calendar_dates_df\n",
    "    \n",
    "    # Import routes\n",
    "    if import_routes == True:\n",
    "        print(\"Loading routes\")\n",
    "        routes_df = pd.read_csv(os.path.join(gtfs_dir, 'routes.txt'), dtype={\n",
    "                'route_id': 'str',  \n",
    "                'agency_id': 'str',  \n",
    "                'route_short_name': 'str',  \n",
    "                'route_long_name': 'str', \n",
    "                'route_desc': 'str', \n",
    "                'route_type': 'Int64',\n",
    "                'route_url': 'str',\n",
    "                'route_color': 'str',  \n",
    "                'route_text_color': 'str', \n",
    "                'exact_times': 'bool'\n",
    "            })\n",
    "        output['routes'] = routes_df\n",
    "    \n",
    "    # Import shapes\n",
    "    if import_shapes == True:\n",
    "        print(\"Loading shapes\")\n",
    "        shapes_df = pd.read_csv(os.path.join(gtfs_dir, \"shapes.txt\"), dtype={\n",
    "                'shape_id': 'str', \n",
    "                'shape_pt_lat': 'float', \n",
    "                'shape_pt_lon': 'float',  \n",
    "                'shape_pt_sequence': 'Int64',\n",
    "                'shape_dist_traveled': 'float',\n",
    "            })\n",
    "        output['shapes'] = shapes_df\n",
    "\n",
    "    # Import stops\n",
    "    if import_stops == True:\n",
    "        print(\"Loading stops\")\n",
    "        stops_df = pd.read_csv(os.path.join(gtfs_dir, \"stops.txt\"), dtype={\n",
    "                'stop_id': 'str', \n",
    "                'stop_name': 'str',\n",
    "                'stop_lat': 'float', \n",
    "                'stop_lon': 'float',  \n",
    "                'location_type': 'Int64', \n",
    "                'parent_station': 'str',\n",
    "            })\n",
    "        output[\"stops\"] = stops_df\n",
    "\n",
    "    # Import stop_times\n",
    "    if import_stoptimes == True:\n",
    "        print(\"Loading stop times\")\n",
    "        stoptimes_df = pd.read_csv(os.path.join(gtfs_dir, \"stop_times.txt\"), dtype={\n",
    "                'trip_id': 'str',\n",
    "                'stop_id': 'str', \n",
    "                'stop_name': 'str',\n",
    "                'arrival_time': 'str',\n",
    "                'departure_time': 'str',\n",
    "                'stop_sequence': 'Int64'\n",
    "            })\n",
    "        output[\"stoptimes\"] = stoptimes_df\n",
    "    \n",
    "    # Import trips\n",
    "    if import_trips == True:\n",
    "        print(\"Loading trips\")\n",
    "        trips_df = pd.read_csv(os.path.join(gtfs_dir, \"trips.txt\"), dtype={\n",
    "            'route_id': 'str', \n",
    "            'service_id': 'str',  \n",
    "            'trip_id': 'str',\n",
    "            'shape_id': 'str', \n",
    "            'trip_headsign': 'str', \n",
    "            'direction_id': 'str',  \n",
    "            'block_id': 'str', \n",
    "            'wheelchair_accessible': 'str', \n",
    "            'route_direction': 'str', \n",
    "            'trip_note': 'str', \n",
    "            'bikes_allowed': 'str'\n",
    "        })\n",
    "        output['trips'] = trips_df\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9353eaac-1d93-4dfa-beb7-9eda4d3a8cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_bundle(\n",
    "    bundle,\n",
    "    import_trips=True,\n",
    "    import_shapes=True,\n",
    "    import_stops=True,\n",
    "    import_stoptimes=True,\n",
    "    cache_bundle=False,\n",
    "):\n",
    "    \"\"\"Import GTFS data from the following tables in ADLS:\n",
    "    - core.dbo.fact_gtfs_trips\n",
    "    - core.dbo.fact_gtfs_shapes\n",
    "    - core.dbo.fact_gtfs_stops\n",
    "    - core.dbo.fact_gtfs_shape_reference\n",
    "    - core.dbo.fact_gtfs_stop_times\n",
    "    \"\"\"\n",
    "\n",
    "    # GTFS bundle can take some time to import. If there is already a cached GTFS bundle pickle file in the repository, read in that data instead\n",
    "    picklefile = bundle + \".pickle\"\n",
    "    if os.path.isfile(picklefile) == True:\n",
    "        with open(picklefile, \"rb\") as handle:\n",
    "            gtfs_bundle = pickle.load(handle)\n",
    "            return gtfs_bundle\n",
    "\n",
    "    con = create_engine(\n",
    "        r\"trino://trino-route-trino.apps.mtasiprod.eastus.aroapp.io:443/mtadatalake\",\n",
    "        connect_args={\n",
    "            \"auth\": OAuth2Authentication(),\n",
    "            \"http_scheme\": \"https\",\n",
    "        }\n",
    "    )\n",
    "    cur = con.connect()\n",
    "    \n",
    "    output = {}\n",
    "    # Import trips\n",
    "    trip_sql = f\"\"\"\n",
    "    with agency as ( -- get agency id for each route\n",
    "        select distinct route_id, agency_id from mtadatalake.core.fact_gtfs_routes \n",
    "        where bundle = '{bundle}'\n",
    "        )\n",
    "    SELECT trips.route_id ,trip_id, service_id,trip_headsign,direction_id,block_id,shape_id,boarding_type,bundle, agency.agency_id\n",
    "    FROM mtadatalake.core.fact_gtfs_trips trips\n",
    "    join agency on trips.route_id = agency.route_id\n",
    "    where trips.bundle = '{bundle}'\n",
    "    \"\"\"\n",
    "    if import_trips == True:\n",
    "        print('Loading trips')\n",
    "        f = cur.execute(text(trip_sql))\n",
    "        trips_df = pd.DataFrame(f.fetchall())\n",
    "        output[\"trips\"] = trips_df\n",
    "\n",
    "    # Import shapes\n",
    "    shape_sql = f\"\"\"\n",
    "    SELECT shape_id, shape_pt_sequence, shape_pt_lat, shape_pt_lon, bundle\n",
    "    FROM mtadatalake.core.fact_gtfs_shapes\n",
    "    where bundle = '{bundle}'\n",
    "    \"\"\"\n",
    "    if import_shapes == True:\n",
    "        print(\"Loading shapes\")\n",
    "        f = cur.execute(text(shape_sql))\n",
    "        shapes_df = pd.DataFrame(f.fetchall())\n",
    "        output[\"shapes\"] = shapes_df\n",
    "\n",
    "    # Import stops\n",
    "    stop_sql = f\"\"\"\n",
    "    SELECT fact_gtfs_stops.stop_id, stop_name, stop_lat, stop_lon, shape_ref.revenue_stop, bundle\n",
    "    FROM mtadatalake.core.fact_gtfs_stops\n",
    "    left join (\n",
    "        SELECT stop_id, MAX(revenue_stop) AS revenue_stop\n",
    "        FROM mtadatalake.core.fact_gtfs_shape_reference\n",
    "        where bundle = '{bundle}'\n",
    "        GROUP BY stop_id\n",
    "    ) shape_ref\n",
    "    on fact_gtfs_stops.stop_id = shape_ref.stop_id\n",
    "    where bundle = '{bundle}'\n",
    "    \"\"\"\n",
    "    if import_stops == True:\n",
    "        print(\"Loading stops\")\n",
    "        f = cur.execute(text(stop_sql))\n",
    "        stops_df = pd.DataFrame(f.fetchall())\n",
    "        output[\"stops\"] = stops_df\n",
    "\n",
    "    # Import stop_times\n",
    "    stoptime_sql = f\"\"\"\n",
    "    SELECT trip_id, stop_id, arrival_time, departure_time, timepoint, stop_sequence, pickup_type, drop_off_type, bundle\n",
    "    FROM mtadatalake.core.fact_gtfs_stop_times\n",
    "    where bundle = '{bundle}'\n",
    "    \"\"\"\n",
    "    if import_stoptimes == True:\n",
    "        print(\"Loading stop times\")\n",
    "        f = cur.execute(text(stoptime_sql))\n",
    "        stoptimes_df = pd.DataFrame(f.fetchall())\n",
    "        output[\"stoptimes\"] = stoptimes_df\n",
    "\n",
    "    if cache_bundle == True:\n",
    "        with open(picklefile, \"wb\") as handle:\n",
    "            pickle.dump(output, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(f\"GTFS data for {bundle} successfully loaded and cached\")\n",
    "    else:\n",
    "       print(f\"GTFS data for {bundle} successfully loaded\") \n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d27f336-1002-4bbf-9060-2f85f5340108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_representative(bundle):\n",
    "    trino = dbhook.Hook(adls_trino)\n",
    "\n",
    "    representative_day_query = '''\n",
    "    with bundle_dates AS (-- Get a list of dates and bundles\n",
    "      SELECT service_date, format_datetime(service_date, 'EEEE') day_of_week, bundle, pick_year, pick_name, sched_type, manual\n",
    "      FROM mtadatalake.core.dim_bus_gtfs_bundle_dates\n",
    "      where bundle in ('2024Jan_Prod_r01_b03_Predate_Shuttles_v2_i1_scheduled', '2024April_Prod_r01_b07_Predate_02_Shuttles_2_SCHEDULED')\n",
    "      --  where pick_year in (2024) \n",
    "    )\n",
    "    , gtfs_calendar_fixed_dates as (\n",
    "      -- Correct invalid dates like February 31st\n",
    "      select service_id, monday, tuesday, wednesday, thursday, friday, saturday, sunday, bundle\n",
    "        , start_date/10000 start_yr, start_date/100 % 100 start_mnth\n",
    "        -- Take out dates like Feb 31 or Apr 31\n",
    "        , case \n",
    "          when start_date/100 % 100 = 2 and start_date % 100 > 28 then 28 \n",
    "          when (start_date/10000) % 4 = 0 and start_date % 100 > 29 then 29 -- if it's a leap year, 2/29\n",
    "          when start_date/100 % 100 in (9, 4, 6, 11) and start_date % 100 > 30 then 30\n",
    "          else start_date % 100\n",
    "        end as start_day\n",
    "        , end_date/10000 end_yr, end_date/100 % 100 end_mnth\n",
    "        , case\n",
    "          when end_date/100 % 100 = 2 and end_date % 100 > 28 then 28\n",
    "          when (end_date/10000) % 4 = 0 and end_date % 100 > 29 then 29 -- if it's a leap year, 2/29\n",
    "          when end_date/100 % 100 in (9, 4, 6, 11) and end_date % 100 > 30 then 30\n",
    "          else end_date % 100\n",
    "        end as end_day\n",
    "      from mtadatalake.core.fact_gtfs_calendar\n",
    "      where bundle in (select distinct bundle from bundle_dates)\n",
    "    )\n",
    "    , calendar as (\n",
    "      -- convert start_date to date object\n",
    "      select service_id, monday, tuesday, wednesday, thursday, friday, saturday, sunday, bundle\n",
    "        , DATE(cast(start_yr AS VARCHAR) || '-' || CAST(start_mnth AS VARCHAR) || '-' || CAST(start_day AS VARCHAR)) as start_date\n",
    "        , DATE(CAST(end_yr AS VARCHAR) || '-' || CAST(end_mnth AS VARCHAR) || '-' || CAST(end_day AS VARCHAR)) AS end_date\n",
    "      from gtfs_calendar_fixed_dates\n",
    "    )\n",
    "    , exceptions as (\n",
    "      SELECT service_id, \"date\", exception_type, bundle\n",
    "        , DATE(cast((\"date\" / 10000) as varchar) || '-' || cast(((\"date\" / 100) % 100) as varchar) || '-' || cast((\"date\" % 100) as varchar)) AS service_date\n",
    "        , format_datetime(DATE(cast((\"date\" / 10000) as varchar) || '-' || cast(((\"date\" / 100) % 100) as varchar) || '-' || cast((\"date\" % 100) as varchar)), 'EEEE') as day_of_week\n",
    "      FROM mtadatalake.core.fact_gtfs_calendar_dates\n",
    "      where bundle in (select distinct bundle from bundle_dates)\n",
    "    )\n",
    "    , base_schedule as (\n",
    "      -- for each day, assemble a row for each service id serving it\n",
    "      SELECT bd.service_date, bd.day_of_week, c.service_id, c.bundle, e.exception_type\n",
    "      FROM bundle_dates bd\n",
    "      inner join calendar c\n",
    "        ON bd.bundle = c.bundle \n",
    "        and bd.service_date BETWEEN c.start_date AND c.end_date\n",
    "        AND (\n",
    "          (bd.day_of_week = 'Monday' AND c.monday = 1) OR\n",
    "          (bd.day_of_week = 'Tuesday' AND c.tuesday = 1) OR\n",
    "          (bd.day_of_week = 'Wednesday' AND c.wednesday = 1) OR\n",
    "          (bd.day_of_week = 'Thursday' AND c.thursday = 1) OR\n",
    "          (bd.day_of_week = 'Friday' AND c.friday = 1) OR\n",
    "          (bd.day_of_week = 'Saturday' AND c.saturday = 1) OR\n",
    "          (bd.day_of_week = 'Sunday' AND c.sunday = 1)\n",
    "          )\n",
    "      -- Join in dates and service_ids where service was removed (exception_type 2)\n",
    "      left join (select * from exceptions where exception_type = 2) e\n",
    "        on c.bundle = e.bundle\n",
    "        and bd.service_date = e.service_date\n",
    "        and c.service_id = e.service_id\n",
    "    )\n",
    "    , modified_schedules as (\n",
    "      -- Join in dates and service_ids where service was added (exception_type 1)\n",
    "      select service_date, day_of_week, service_id, bundle, exception_type\n",
    "      from base_schedule \n",
    "      where exception_type is null or exception_type = 1\n",
    "      union all\n",
    "      select service_date, day_of_week, service_id, bundle, exception_type\n",
    "      from exceptions \n",
    "      where exception_type = 1\n",
    "    )\n",
    "    , daily_schedules as (\n",
    "      -- Designate schedule daytype\n",
    "      select service_date, day_of_week, service_id, bundle\n",
    "        , CASE \n",
    "            WHEN day_of_week(service_date) BETWEEN 1 AND 5 THEN 'Weekday'\n",
    "            WHEN day_of_week(service_date) = 6 THEN 'Saturday'\n",
    "            WHEN day_of_week(service_date) = 7 THEN 'Sunday'\n",
    "            ELSE null\n",
    "          end as sched_daytype\n",
    "      from modified_schedules\n",
    "    )\n",
    "    , schedules_per_day AS (\n",
    "        -- Aggregate service_ids for each day to form a \"schedule\"\n",
    "        SELECT \n",
    "            bundle, service_date, sched_daytype\n",
    "            , array_join(array_agg(CAST(service_id AS VARCHAR) ORDER BY service_id), ',') AS schedule\n",
    "        FROM daily_schedules\n",
    "        GROUP BY bundle, service_date, sched_daytype\n",
    "    )\n",
    "    , schedule_variations AS (\n",
    "      -- Count the occurrences of each unique schedule\n",
    "      SELECT bundle, service_date, schedule, sched_daytype\n",
    "        , COUNT(service_date) over (partition by bundle, schedule, sched_daytype) AS sched_var_frequency\n",
    "        -- Make a name for each unique schedule, e.g. Weekday-1, Saturday-3\n",
    "        ,  sched_daytype || '-' || CAST(DENSE_RANK() OVER (PARTITION BY bundle, sched_daytype ORDER BY schedule) AS VARCHAR) AS schedule_variation\n",
    "      FROM schedules_per_day\n",
    "    )\n",
    "    , schedule_variations_ranked as ( -- this CTE returns one row per day, with the schedule variation and rank\n",
    "      select bundle, service_date, schedule, sched_daytype, schedule_variation, sched_var_frequency\n",
    "        , row_number() over (partition by bundle, sched_daytype order by sched_var_frequency desc, service_date) ranking_by_freq\n",
    "      from schedule_variations\n",
    "    )\n",
    "    , most_representative_day as ( -- this CTE will give a weekday, saturday, and sunday date for each bundle\n",
    "      -- When ranking_by_freq = 1, that day is the best representation of a weekday, saturday, or sunday schedule\n",
    "        select bundle, service_date, sched_daytype as day_of_week\n",
    "        from schedule_variations_ranked\n",
    "        where ranking_by_freq = 1\n",
    "    )\n",
    "    -- decide between schedule_variations_ranked, most_representative_day, or sched_with_service_id\n",
    "    select * from most_representative_day\n",
    "    '''\n",
    "    return trino.frame(representative_day_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74224e2f-5325-4022-be1c-f75bc9221e1f",
   "metadata": {},
   "source": [
    "### load all GTFS files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1583306c-b150-4ea1-aa58-aaec7fd8aa53",
   "metadata": {},
   "source": [
    "### GTFS1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e83eb8c6-8fd7-4322-b96a-c40692f1b482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trips\n",
      "Open the following URL in browser for the external authentication:\n",
      "https://trino-route-trino.apps.mtasiprod.eastus.aroapp.io/oauth2/token/initiate/c120148181b7ff92591be1b51a0bbd8beaa8103b68b905038b9f05a497397747\n",
      "Loading shapes\n",
      "Loading stops\n",
      "Loading stop times\n",
      "GTFS data for 2025June_Prod_r04_b01_PREDATE_SHUTTLES_SCHEDULED successfully loaded\n"
     ]
    }
   ],
   "source": [
    "gtfs1 = import_bundle('2025June_Prod_r04_b01_PREDATE_SHUTTLES_SCHEDULED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93d19596-26e6-4f0b-ad35-9cdf62b1321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips1 = gtfs1['trips']\n",
    "stoptimes1 = gtfs1['stoptimes']\n",
    "stops1 = gtfs1['stops']\n",
    "\n",
    "#joining trips to stoptimes on trip_id, then to stops on stop_id to get stop_name\n",
    "joined1 = pd.merge(pd.merge(trips1, stoptimes1, how = 'outer', on = 'trip_id'), stops1, on = 'stop_id')\n",
    "\n",
    "#getting only the columns asked for and dropping duplicates\n",
    "stoplist1 = joined1[['route_id','direction_id','stop_id','stop_name']].drop_duplicates(subset = ['route_id','direction_id','stop_id'])\n",
    "\n",
    "#sorting by route_id, direction_id and stop_id\n",
    "sortedstoplist1 = stoplist1.sort_values(by=['route_id','direction_id','stop_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14de3ce8-1d16-4101-982c-149468ff78f7",
   "metadata": {},
   "source": [
    "### GTFS 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa26f6f5-1dbe-43d8-b99e-78541fc6dd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trips\n",
      "Loading shapes\n",
      "Loading stops\n",
      "Loading stop times\n",
      "GTFS data for 2025March_Prod_r01_b05_SHUTTLES_PREDATE_SCHEDULED successfully loaded\n"
     ]
    }
   ],
   "source": [
    "gtfs2 = import_bundle('2025March_Prod_r01_b05_SHUTTLES_PREDATE_SCHEDULED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de4f2edd-d8d5-435a-aeed-29baabad441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips2 = gtfs2['trips']\n",
    "stoptimes2 = gtfs2['stoptimes']\n",
    "stops2 = gtfs2['stops']\n",
    "\n",
    "#joining trips to stoptimes on trip_id, then to stops on stop_id to get stop_name\n",
    "joined2 = pd.merge(pd.merge(trips2, stoptimes2, how = 'outer', on = 'trip_id'), stops2, on = 'stop_id')\n",
    "\n",
    "#getting only the columns asked for and dropping duplicates\n",
    "stoplist2 = joined2[['route_id','direction_id','stop_id','stop_name']].drop_duplicates(subset = ['route_id','direction_id','stop_id'])\n",
    "\n",
    "#sorting by route_id, direction_id and stop_id\n",
    "sortedstoplist2 = stoplist2.sort_values(by=['route_id','direction_id','stop_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1214b224-25ba-4acb-8ebb-5b476b147f0a",
   "metadata": {},
   "source": [
    "#### Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a1d97b5-7714-4d2b-ab9a-c7675b53efa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on route_id, direction_id, stop_id\n",
    "merged = pd.merge(sortedstoplist1, sortedstoplist2, how='outer', on=['route_id', 'direction_id','stop_id'], suffixes=('_b1', '_b2'))\n",
    "\n",
    "# Reorder and rename columns\n",
    "final = merged[[\n",
    "    'route_id', 'direction_id',\n",
    "    'stop_id', 'stop_name_b1',\n",
    "    'stop_id', 'stop_name_b2'\n",
    "]]\n",
    "\n",
    "# Fill missing values with empty string\n",
    "final = final.fillna('')\n",
    "final.sort_values(by=['route_id','direction_id'])\n",
    "\n",
    "\n",
    "final.to_csv(\"all_stops_combined1.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baba520e-a0d8-47a9-a0bd-8f01893ee9d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
